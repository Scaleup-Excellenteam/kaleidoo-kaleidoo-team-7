{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Research**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Input parsing:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Audio Transcription**\n",
    "\n",
    "1.   Installing the `faster_whisper` Library\n",
    "2.   Importing Necessary Libraries\n",
    "3.   Determining the Computing Device\n",
    "4.   Selecting the Model Size and Compute Type\n",
    "5.   Loading the Whisper Model\n",
    "6.   Transcribing the Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install faster_whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_size = \"small\"\n",
    "\n",
    "if device == \"cpu\":\n",
    "    compute_type = \"int8\"\n",
    "else:\n",
    "    compute_type = \"float16\"\n",
    "\n",
    "\n",
    "model = WhisperModel(model_size, device=device, compute_type=compute_type)\n",
    "\n",
    "audio_file = './america.wav'\n",
    "\n",
    "segments, info = model.transcribe(audio_file, beam_size=1)\n",
    "\n",
    "transcription = \"\"\n",
    "for segment in segments:\n",
    "    transcription += segment.text\n",
    "\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load BERT and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Video Processing: Object Detection and Audio Transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this notebook, we will process several video clips to:\n",
    "\n",
    "- Perform object detection using different models.\n",
    "- Transcribe audio using different transcription tools.\n",
    "- Measure performance metrics such as processing speed, detection accuracy, and transcription quality.\n",
    "- Compare the performance of different tools.\n",
    "\n",
    "**Tools Used:**\n",
    "\n",
    "- **Object Detection Models**:\n",
    "  - YOLOv8 (Ultralytics)\n",
    "  - YOLOv5 (Ultralytics) \n",
    "  - BLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Install necessary packages for document parsing, YOLO object detection, BLIP and data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Install Ultralytics for YOLOv8 and YOLOv5\n",
    "!pip install ultralytics\n",
    "\n",
    "# Install OpenCV for video processing\n",
    "!pip install opencv-python\n",
    "\n",
    "# Install pandas for data manipulation\n",
    "!pip install pandas\n",
    "\n",
    "# Install matplotlib and seaborn for visualization\n",
    "!pip install matplotlib seaborn\n",
    "\n",
    "# Install PyTorch and related libraries for deep learning\n",
    "!pip install torch torchvision torchaudio\n",
    "\n",
    "# Install Hugging Face Transformers for NLP and computer vision models\n",
    "!pip install transformers\n",
    "\n",
    "# Install Hugging Face Datasets for data handling\n",
    "!pip install datasets\n",
    "\n",
    "# Install tqdm for progress bars\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Import necessary libraries and AI models for , object detection, video processing, and data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import matplotlib.pyplot as plt\n",

    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Data for analysing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List of video files\n",
    "video_files = [\n",
    "    'SourcesTests/fruit-and-vegetable-detection.mp4',  # High resolution, good lighting\n",
    "    'SourcesTests/traffic-mini.mp4',  # Medium resolution, low lighting\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Function to extract frames from the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_frames(video_path, frame_interval=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    success, frame = cap.read()\n",
    "    count = 0\n",
    "\n",
    "    while success:\n",
    "        if count % frame_interval == 0:\n",
    "            frames.append(frame)\n",
    "        success, frame = cap.read()\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "# Example usage\n",
    "video_path = 'SourcesTests/fruit-and-vegetable-detection.mp4'  # Update with your video path\n",
    "frames = extract_frames(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load the BLIP processor and model for Video Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to perform Video Question Answering using BLIP\n",
    "def blip_video_qa(frames, question):\n",
    "    answers = []\n",
    "\n",
    "    for i, frame in enumerate(frames):\n",
    "        # Convert frame to PIL Image\n",
    "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        # Prepare inputs for the model\n",
    "        inputs = processor(image, question, return_tensors=\"pt\")\n",
    "\n",
    "        # Generate the answer\n",
    "        output = model.generate(**inputs)\n",
    "        answer = processor.decode(output[0], skip_special_tokens=True)\n",
    "        answers.append((i, answer))  # Store frame index and answer\n",
    "\n",
    "    return answers\n",
    "\n",
    "# Define your question\n",
    "question = \"When in the video does it show the apple for the first time?\"\n",
    "\n",
    "# Perform Video QA using BLIP\n",
    "blip_answers = blip_video_qa(frames, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " Measure processing time for BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "blip_answers = blip_video_qa(frames, question)\n",
    "blip_processing_time = time.time() - start_time\n",
    "\n",
    "print(f\"BLIP Processing Time: {blip_processing_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Function for Object Detection in Videos Using YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import io\n",
    "def yolo_v8_detection(video_path, user_object, frame_interval=5):\n",
    "    model = YOLO('yolov8s.pt')  # Load the YOLOv8 model\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = 0\n",
    "    timestamps = []\n",
    "    processing_times = []\n",
    "    class_names = model.names\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count += 1\n",
    "    \n",
    "            if frame_count % frame_interval != 0:\n",
    "                continue\n",
    "    \n",
    "            start_time = time.time()\n",
    "            results = model(frame)\n",
    "            end_time = time.time()\n",
    "            processing_times.append(end_time - start_time)\n",
    "    \n",
    "            for result in results:\n",
    "                boxes = result.boxes\n",
    "                for box in boxes:\n",
    "                    class_id = int(box.cls[0])\n",
    "                    class_name = class_names[class_id]\n",
    "                    if class_name == user_object:\n",
    "                        timestamp = frame_count / fps\n",
    "                        timestamps.append(timestamp)\n",
    "                        break\n",
    "\n",
    "    cap.release()\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "\n",
    "    return {\n",
    "        'timestamps': sorted(set(timestamps)),\n",
    "        'processing_time': avg_processing_time,\n",
    "        'frames_processed': frame_count // frame_interval\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Function for Object Detection in Videos Using YOLOv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import io\n",
    "def yolo_v5_detection(video_path, user_object, frame_interval=5):\n",
    "    model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = 0\n",
    "    timestamps = []\n",
    "    processing_times = []\n",
    "    class_names = model.names\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count += 1\n",
    "    \n",
    "            if frame_count % frame_interval != 0:\n",
    "                continue\n",
    "    \n",
    "            start_time = time.time()\n",
    "            results = model(frame)\n",
    "            end_time = time.time()\n",
    "            processing_times.append(end_time - start_time)\n",
    "    \n",
    "            # Parse detection results\n",
    "            labels = results.xyxy[0][:, -1].cpu().numpy()\n",
    "            for label in labels:\n",
    "                class_name = class_names[int(label)]\n",
    "                if class_name == user_object:\n",
    "                    timestamp = frame_count / fps\n",
    "                    timestamps.append(timestamp)\n",
    "                    break\n",
    "\n",
    "    cap.release()\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "\n",
    "    return {\n",
    "        'timestamps': sorted(set(timestamps)),\n",
    "        'processing_time': avg_processing_time,\n",
    "        'frames_processed': frame_count // frame_interval\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Script for Running Object Detection and Video Question Answering with YOLOv8, YOLOv5, and BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_user_objects = {\n",
    "    'SourcesTests/fruit-and-vegetable-detection.mp4': 'apple',\n",
    "    'SourcesTests/traffic-mini.mp4': 'truck',\n",
    "    # Add more mappings if needed\n",
    "}\n",
    "# List to store results\n",
    "results_list = []\n",
    "\n",
    "for video in video_files:\n",
    "    print(f\"Processing {video}...\")\n",
    "    \n",
    "    user_object = video_user_objects.get(video, None)\n",
    "    if user_object is None:\n",
    "        print(f\"No user_object specified for {video}. Skipping this video.\")\n",
    "        continue\n",
    "    \n",
    "    # YOLOv8 Detection\n",
    "    yolo_v8_results = yolo_v8_detection(video, user_object)\n",
    "    \n",
    "    \n",
    "    results_list.append({\n",
    "        'Video': video,\n",
    "        'Tool': 'YOLOv8',\n",
    "        'Processing Time (s)': yolo_v8_results['processing_time'],\n",
    "        'Frames Processed': yolo_v8_results['frames_processed']\n",
    "    })\n",
    "    \n",
    "    # YOLOv5 Detection\n",
    "    yolo_v5_results = yolo_v5_detection(video, user_object)\n",
    "  \n",
    "    \n",
    "    results_list.append({\n",
    "        'Video': video,\n",
    "        'Tool': 'YOLOv5',\n",
    "        'Processing Time (s)': yolo_v5_results['processing_time'],\n",
    "        'Frames Processed': yolo_v5_results['frames_processed']\n",
    "    })\n",
    "    # BLIP Detection (using Video QA)\n",
    "    start_time = time.time()\n",
    "    blip_answers = blip_video_qa(extract_frames(video), \"When in the video its show on the first time the apple?\")\n",
    "    blip_processing_time = time.time() - start_time\n",
    "\n",
    "    results_list.append({\n",
    "        'Video': video,\n",
    "        'Tool': 'BLIP',\n",
    "        'Processing Time (s)': blip_processing_time,\n",
    "        'Frames Processed': len(frames)\n",
    "    })\n",
    "    results_df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Script for Displaying and Visualizing Object Detection and Video QA Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Ensure all columns are displayed\n",
    "pd.set_option('display.expand_frame_repr', False)  # Avoid line breaks in the table display\n",
    "pd.set_option('display.colheader_justify', 'center')  # Center-align column headers\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(\"\\nSummary of Processing Results:\")\n",
    "print(results_df.to_string(index=False))  # Display the DataFrame in a readable format\n",
    "\n",
    "# Plot Processing Time Comparison for YOLOv8, YOLOv5, and BLIP\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=results_df, x='Tool', y='Processing Time (s)', hue='Video')\n",
    "plt.title('Processing Time Comparison for YOLOv8, YOLOv5, and BLIP')\n",
    "plt.xlabel('Detection Tool')\n",
    "plt.ylabel('Processing Time (seconds)')\n",
    "plt.legend(title='Video File', loc='upper right')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()  # Adjust layout for better fit\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Retrieval-Augmented Generation (RAG)**\n",
    "\n",
    "This script is designed to identify and retrieve the most relevant documents from a predefined corpus based on a user-provided query. It leverages the power of Sentence Transformers to convert text into meaningful vector embeddings and uses cosine similarity from scikit-learn to measure the semantic similarity between the query and the documents. Additionally, the script includes multilingual support by incorporating documents in Hebrew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Text Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.   Installing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence_transformers scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.    Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.   Loading the Sentence Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Defining the Corpus of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =[ \"\"\"\n",
    "    Artificial Intelligence (AI) has transformed the healthcare industry by offering new ways to diagnose,\n",
    "    treat, and manage diseases. AI algorithms, particularly deep learning, are being used to analyze medical\n",
    "    images, predict disease outbreaks, and personalize treatment plans. The integration of AI in healthcare has\n",
    "    reduced human error, improved accuracy, and increased the efficiency of medical professionals. AI-powered\n",
    "    robots are assisting surgeons in complex procedures, while predictive analytics is helping doctors make\n",
    "    more informed decisions. Despite these advances, there are challenges such as data privacy and the need\n",
    "    for comprehensive validation of AI models before widespread adoption.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Cloud computing has revolutionized the way businesses operate, offering flexible and scalable infrastructure\n",
    "    that can adjust to their needs. By moving to the cloud, companies no longer need to invest heavily in\n",
    "    on-premise hardware. Instead, they can access powerful computing resources over the internet, enabling them\n",
    "    to focus on innovation and growth. Businesses use cloud services for data storage, application hosting, and\n",
    "    collaboration, benefiting from reduced costs, enhanced security, and improved accessibility. However, concerns\n",
    "    about data breaches and vendor lock-in persist, as companies need to carefully select cloud providers to\n",
    "    ensure long-term sustainability.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Neural networks, a fundamental building block of artificial intelligence, have evolved significantly since their\n",
    "    inception. Initially inspired by the human brain, neural networks are designed to mimic the way neurons in\n",
    "    the brain process information. Over the years, advances in deep learning, a subset of neural networks, have made\n",
    "    it possible for AI systems to achieve unprecedented levels of accuracy in tasks like image recognition, natural\n",
    "    language processing, and autonomous driving. Neural networks are composed of layers of interconnected nodes,\n",
    "    where each node represents a neuron. The training of neural networks involves adjusting weights based on input\n",
    "    data, allowing the model to learn patterns and make predictions. Despite their success, training large neural\n",
    "    networks requires significant computational power and data.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    As the world faces the growing threat of climate change, sustainable energy has become a major focus of global\n",
    "    efforts. Renewable energy sources such as solar, wind, and hydropower are being developed to reduce dependence\n",
    "    on fossil fuels. Clean technologies are playing a critical role in achieving sustainability goals, with innovations\n",
    "    in energy storage, electric vehicles, and smart grids leading the way. Governments and private companies alike are\n",
    "    investing heavily in research and development to create more efficient and cost-effective solutions. While the\n",
    "    transition to sustainable energy presents challenges, including the initial cost of infrastructure and the need for\n",
    "    reliable energy storage, it also offers immense benefits in terms of reducing greenhouse gas emissions and creating\n",
    "    new economic opportunities.\n",
    "    \"\"\",\n",
    "     \"\"\"\n",
    "    מאמר 1: השפעת הבינה המלאכותית על תהליכים משפטיים\n",
    "בעשור האחרון, הבינה המלאכותית (AI) הפכה להיות כלי מרכזי בתהליכים משפטיים, בין אם במתן ייעוץ משפטי אוטומטי ובין אם בניתוח מסמכים משפטיים. אחד היתרונות המרכזיים של AI הוא היכולת לנתח כמויות גדולות של מידע במהירות וביעילות. מערכות בינה מלאכותית מסוגלות לעבד חומרים משפטיים מורכבים, כגון חוזים, תיקים משפטיים, ופסיקות עבר, ולספק תחזיות על בסיס נתונים היסטוריים. שימוש ב-AI מבטיח יעילות משופרת, חיסכון בזמן והפחתת טעויות אנוש בתהליכי קבלת החלטות. עם זאת, השימוש הגובר ב-AI מעלה שאלות אתיות ומשפטיות רבות. לדוגמה, נושאים של אחריות משפטית: האם ניתן לסמוך על מערכת אוטומטית בקבלת החלטות משפטיות? שאלות אלו מתחדדות בעיקר בתחום הפלילי, כאשר טכנולוגיות אלו משמשות ככלי לסיוע בקביעת עונשים או החלטות שיפוטיות אחרות. אחת מהשאלות המרכזיות הנוגעות ל-AI בתחום המשפט היא האם ניתן להחליט על סוגיות משפטיות בהתבסס על ניתוח של אלגוריתם בלבד, או שיש צורך באישוש ההחלטות על ידי בני אדם. כמו כן, בעידן של רגולציה טכנולוגית מוגברת, חשוב לשאול האם המערכות הללו עומדות בקריטריונים של שקיפות והוגנות, והאם ישנם מנגנונים שימנעו מהן לבצע הטיות סמויות בהחלטות שהן מקבלות. יתר על כן, השימוש ב-AI בחוזים מסחריים הפך להיות שכיח ביותר, כאשר מערכות אוטומטיות יכולות לסייע בניתוח וכתיבה של חוזים. עם זאת, יש צורך בהתאמת החקיקה כדי להבטיח שהחוזים שנוצרים על ידי מערכות אלו יעמדו בדרישות החוק. בתי המשפט נדרשים להתמודד עם פסיקות הנוגעות לחוזים שנוצרו או נחתמו באמצעות AI, וחקיקה שתסדיר נושא זה עדיין מתהווה.\n",
    "      \"\"\",\n",
    "      \"\"\"\n",
    "    מאמר 2: משפט בינלאומי וזכויות אדם בעידן הגלובליזציה\n",
    "    המשפט הבינלאומי מתמודד כיום עם אתגרים חדשים הנובעים מהתהליך המואץ של גלובליזציה והשפעתה על זכויות אדם. בעידן של תנועת אנשים, הון, וסחורות בצורה חסרת תקדים, יש צורך בהסדרים משפטיים בינלאומיים המבטיחים הגנה על זכויות האדם בכל המדינות. אמנות בינלאומיות, כגון האמנה הבינלאומית לזכויות האדם, נועדו להבטיח שכל אדם יקבל את ההגנות הנדרשות גם כאשר הוא חוצה גבולות. עם זאת, במקרים רבים נתקלים במצבים בהם יש פער בין הרטוריקה הבינלאומית לזכויות האדם לבין היישום המעשי שלהן. לדוגמה, נושא של פליטים ומהגרים הוא נושא קריטי במשפט הבינלאומי. בעוד שהאמנה לזכויות האדם מבטיחה הגנה לכל אדם, המציאות היא שמדינות רבות מוצאות דרכים לעקוף את ההתחייבויות הבינלאומיות שלהן בנוגע לפליטים, בשל שיקולים כלכליים ופוליטיים. סוגיה נוספת שעולה היא השימוש בטכנולוגיות חדשות בתחום הביטחון והמעקב, כמו מערכות זיהוי פנים ואלגוריתמים למעקב אחר תנועות אנשים. בעוד שהשימוש בטכנולוגיות אלו נועד להגביר את הביטחון, הוא מעלה שאלות אתיות ומשפטיות בנוגע לפגיעה בפרטיות וזכויות אדם. יתר על כן, גם בתחום הכלכלה הגלובלית, נדרשת התייחסות משפטית מיוחדת לנושאים של צדק חברתי ושוויון. תאגידים בינלאומיים, שפועלים במספר מדינות, מתמודדים עם בעיות משפטיות מורכבות הקשורות לזכויות עובדים, הגנה על הסביבה, ומניעת תחרות לא הוגנת. המשפט הבינלאומי נדרש להסדיר תחומים אלו ולהבטיח שהגלובליזציה לא תפגע בזכויות האדם הבסיסיות.\n",
    "    \"\"\",\n",
    "      \"\"\"\n",
    "    מאמר 3: הגנת הפרטיות בעידן הדיגיטלי\n",
    "    הגנת הפרטיות הפכה לאחד הנושאים המשפטיים החשובים ביותר בעידן הדיגיטלי. הטכנולוגיות החדשות, ובמיוחד השימוש באינטרנט ובמדיה החברתית, יצרו מצבים חדשים בהם מידע אישי זורם בצורה מהירה וחסרת גבולות, ולעיתים רבות ללא ידיעת המשתמשים. חברות ענק כמו פייסבוק, גוגל ואמזון אוספות כמויות עצומות של מידע על המשתמשים שלהן, החל מהרגלי גלישה ועד להעדפות צרכניות אישיות. המידע שנאסף מנוצל לעיתים לצרכים מסחריים, כגון פרסום ממוקד, אך הוא גם עשוי להגיע לידי צדדים שלישיים או ממשלות, מה שמעלה את החשש לפגיעה בפרטיות האישית. בעולם בו המידע האישי הפך לנכס כלכלי חשוב, המשפט נדרש לספק כלים חדשים שיבטיחו את ההגנה על פרטיות המשתמשים. אחת הדוגמאות המרכזיות היא החקיקה האירופית בנושא הגנת הפרטיות – ה-GDPR (General Data Protection Regulation). חקיקה זו מעניקה לאזרחים באיחוד האירופי זכויות נרחבות בנוגע לאופן בו המידע האישי שלהם נאסף ומנוהל, כולל הזכות למחוק מידע (\"הזכות להישכח\") והזכות לקבל מידע על מי משתמש במידע האישי שלהם. חוקים דומים נחקקו במדינות נוספות, אולם במקומות רבים בעולם החקיקה בנושא פרטיות עדיין נמצאת בפיגור לעומת התפתחות הטכנולוגיה. נוסף על כך, קיימת שאלה משפטית מעניינת הנוגעת לאחריותן של החברות שמחזיקות במידע. כאשר מתרחשות פריצות אבטחה או דליפות מידע, נדרשת אחריות משפטית מצד החברות, אולם לעיתים קרובות הפיצוי למשתמשים אינו מספק. חקיקה נוספת נדרשת כדי להבטיח שהזכויות הדיגיטליות של המשתמשים יישמרו.\n",
    "    \"\"\",\n",
    "      \"\"\"\n",
    "    מאמר 4: חוזים חכמים וטכנולוגיית הבלוקצ'יין\n",
    "    טכנולוגיית הבלוקצ'יין יצרה מהפכה בשוק הכלכלי והמשפטי, בעיקר באמצעות החוזים החכמים. בלוקצ'יין הוא פרוטוקול מבוזר המאפשר רישום ואימות של עסקאות בצורה מאובטחת וללא צורך בגורם מתווך. החוזים החכמים, הפועלים על גבי רשתות הבלוקצ'יין, הם תוכנות המתבצעות באופן אוטומטי כאשר מתקיימים תנאים מוסכמים בין הצדדים. כלומר, מדובר בחוזים דיגיטליים שיכולים לאכוף את עצמם. לדוגמה, עסקה בין שני צדדים תתבצע אוטומטית כאשר יתקיימו כל התנאים שנקבעו בחוזה החכם, ללא צורך בהתערבות מצד שלישי כמו עורך דין או נוטריון. החוזים החכמים מהווים שינוי תפיסתי עמוק בעולם המשפט, שכן הם מייתרים את הצורך בתהליכי אכיפה מסורתיים. עם זאת, השימוש בחוזים חכמים מעלה שאלות משפטיות רבות. אחת מהשאלות המרכזיות היא מה קורה במצב של טעות או הונאה בחוזה חכם. מאחר והחוזה מתבצע באופן אוטומטי, לא תמיד יש אפשרות לבטל או לשנות את התנאים. כמו כן, קיימת השאלה האם חוזים חכמים עומדים בדרישות החוק הקיימות בכל הנוגע לכשירות חוזית. תחום נוסף הדורש בחינה משפטית הוא האבטחה של רשתות הבלוקצ'יין עצמן. למרות שהבלוקצ'יין נחשב לטכנולוגיה מאובטחת, היו מקרים בהם התרחשה פריצה לרשתות אלו, מה שהוביל לאובדן כספים ונתונים. נושא האבטחה ימשיך להיות נושא מרכזי בעידן הבלוקצ'יין, וככל שיותר גופים מאמצים טכנולוגיה זו, יידרש פיקוח רגולטורי מתאים. בנוסף, יש לבחון את סוגיות המסים הנוגעות לעסקאות המתבצעות באמצעות בלוקצ'יין, שכן חוזים חכמים מציבים אתגרים חדשים במיסוי ובהגדרה המשפטית של הרווחים הנובעים מהם.\n",
    "    \"\"\"\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Vector Database**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generating Embeddings for All Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_embeddings = sbert_model.encode(data, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Defining the Similarity Matching Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_match(query, texts, sbert_model, sbert_embeddings, top_k=2):\n",
    "    \"\"\"\n",
    "    Finds the best matching text(s) for the given query based on cosine similarity.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The user query string.\n",
    "    - texts (List[str]): List of texts to compare against.\n",
    "    - sbert_model (SentenceTransformer): The Sentence Transformers model.\n",
    "    - sbert_embeddings (np.ndarray): Precomputed embeddings of the texts.\n",
    "    - top_k (int): Number of top matches to return.\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples containing (text, similarity_score)\n",
    "    \"\"\"\n",
    "    query_embedding = sbert_model.encode([query], convert_to_numpy=True)\n",
    "\n",
    "    similarities = cosine_similarity(query_embedding, sbert_embeddings)[0]\n",
    "\n",
    "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "\n",
    "    best_matches = [(texts[idx], similarities[idx]) for idx in top_indices]\n",
    "\n",
    "    return best_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. testing the model with query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"מהם החוזים החכמים ואיך טכנולוגיית הבלוקצ'יין משפיעה על התחום המשפטי?\"\n",
    "\n",
    "# Find the best match\n",
    "best_match = find_best_match(query, data, sbert_model, sbert_embeddings, top_k=3)\n",
    "\n",
    "# Display the result\n",
    "print(\"Best Match:\")\n",
    "for text, score in best_match:\n",
    "    print(f\"Text: {text}\\nSimilarity Score: {score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load BERT and Tokenizer - option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T08:31:00.792462Z",
     "start_time": "2024-09-16T08:31:00.112323Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract embeddings from BERT using the [CLS] token, which will serve as the sentence embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T08:31:00.806545Z",
     "start_time": "2024-09-16T08:31:00.796454Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to get the [CLS] embedding for a given sentence\n",
    "def get_cls_embedding(sentence):\n",
    "    # Tokenize the sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "    \n",
    "    # Get the embeddings from the BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the [CLS] token embedding (first token)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "    return cls_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute the similarity between different queries using cosine similarity to show how BERT handles semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T08:31:07.313896Z",
     "start_time": "2024-09-16T08:31:00.809543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between query 1 and query 2: 0.8579674959182739\n",
      "Similarity between query 1 and query 3: 0.8106188774108887\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Get embeddings for similar and different queries\n",
    "query1 = \"Explain the role of backpropagation in deep learning.\"\n",
    "query2 = \"How do convolutional neural networks work?\"\n",
    "query3 = \"What is the process to register for courses at the university?\"\n",
    "\n",
    "embedding1 = get_cls_embedding(query1)\n",
    "embedding2 = get_cls_embedding(query2)\n",
    "embedding3 = get_cls_embedding(query3)\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarity_1_2 = cosine_similarity(embedding1, embedding2)\n",
    "similarity_1_3 = cosine_similarity(embedding1, embedding3)\n",
    "\n",
    "print(f\"Similarity between query 1 and query 2: {similarity_1_2[0][0]}\")\n",
    "print(f\"Similarity between query 1 and query 3: {similarity_1_3[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text retrevial using BERT we embedd the documents with a cls token which respresents the sentence and we then do a similarity check between the query .\n",
    "since the query is about backpropagtion document 1 and 3 should have a higher similarity than doc 2 which is about vpb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T08:31:10.623167Z",
     "start_time": "2024-09-16T08:31:07.320884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity with Document 1: 0.8547440767288208\n",
      "Similarity with Document 2: 0.8254550695419312\n",
      "Similarity with Document 3: 0.8463909029960632\n"
     ]
    }
   ],
   "source": [
    "# Example documents\n",
    "doc1 = \"Neural networks are computing systems inspired by the biological neural networks.\"\n",
    "doc2 = \"To connect to the university VPN, you need to configure your VPN client.\"\n",
    "doc3 = \"Backpropagation is a fundamental algorithm in training deep learning models.\"\n",
    "\n",
    "# Get embeddings for documents\n",
    "doc_embedding1 = get_cls_embedding(doc1)\n",
    "doc_embedding2 = get_cls_embedding(doc2)\n",
    "doc_embedding3 = get_cls_embedding(doc3)\n",
    "\n",
    "# Compare query to documents\n",
    "query = \"How does backpropagation work in neural networks?\"\n",
    "\n",
    "query_embedding = get_cls_embedding(query)\n",
    "\n",
    "# Compute cosine similarities between query and documents\n",
    "similarity_doc1 = cosine_similarity(query_embedding, doc_embedding1)\n",
    "similarity_doc2 = cosine_similarity(query_embedding, doc_embedding2)\n",
    "similarity_doc3 = cosine_similarity(query_embedding, doc_embedding3)\n",
    "\n",
    "# Show results\n",
    "print(f\"Similarity with Document 1: {similarity_doc1[0][0]}\")\n",
    "print(f\"Similarity with Document 2: {similarity_doc2[0][0]}\")\n",
    "print(f\"Similarity with Document 3: {similarity_doc3[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T08:31:12.348492Z",
     "start_time": "2024-09-16T08:31:10.627681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity with Document 1: 0.7179515957832336\n",
      "Similarity with Document 2: 0.5887858271598816\n",
      "Similarity with Document 3: 0.6379864811897278\n"
     ]
    }
   ],
   "source": [
    "# Example documents\n",
    "doc1 = \"Artificial Intelligence (AI) has transformed the healthcare industry by offering new ways to diagnose, treat, and manage diseases. AI algorithms, particularly deep learning, are being used to analyze medical images, predict disease outbreaks, and personalize treatment plans. The integration of AI in healthcare has reduced human error, improved accuracy, and increased the efficiency of medical professionals. AI-powered robots are assisting surgeons in complex procedures, while predictive analytics is helping doctors make more informed decisions. Despite these advances, there are challenges such as data privacy and the need for comprehensive validation of AI models before widespread adoption.\"\n",
    "doc2 = \"Cloud computing has revolutionized the way businesses operate, offering flexible and scalable infrastructure that can adjust to their needs. By moving to the cloud, companies no longer need to invest heavily in on-premise hardware. Instead, they can access powerful computing resources over the internet, enabling them to focus on innovation and growth. Businesses use cloud services for data storage, application hosting, and collaboration, benefiting from reduced costs, enhanced security, and improved accessibility. However, concerns about data breaches and vendor lock-in persist, as companies need to carefully select cloud providers to ensure long-term sustainability.\"\n",
    "doc3 = \"Neural networks, a fundamental building block of artificial intelligence, have evolved significantly since their inception. Initially inspired by the human brain, neural networks are designed to mimic the way neurons in the brain process information. Over the years, advances in deep learning, a subset of neural networks, have made it possible for AI systems to achieve unprecedented levels of accuracy in tasks like image recognition, natural language processing, and autonomous driving. Neural networks are composed of layers of interconnected nodes, where each node represents a neuron. The training of neural networks involves adjusting weights based on input data, allowing the model to learn patterns and make predictions. Despite their success, training large neural networks requires significant computational power and data.\"\n",
    "\n",
    "# Get embeddings for documents\n",
    "doc_embedding1 = get_cls_embedding(doc1)\n",
    "doc_embedding2 = get_cls_embedding(doc2)\n",
    "doc_embedding3 = get_cls_embedding(doc3)\n",
    "\n",
    "# Compare query to documents\n",
    "query = \"How do neural networks function in artificial intelligence, and what are the challenges of training them?\"\n",
    "\n",
    "query_embedding = get_cls_embedding(query)\n",
    "\n",
    "# Compute cosine similarities between query and documents\n",
    "similarity_doc1 = cosine_similarity(query_embedding, doc_embedding1)\n",
    "similarity_doc2 = cosine_similarity(query_embedding, doc_embedding2)\n",
    "similarity_doc3 = cosine_similarity(query_embedding, doc_embedding3)\n",
    "\n",
    "# Show results\n",
    "print(f\"Similarity with Document 1: {similarity_doc1[0][0]}\")\n",
    "print(f\"Similarity with Document 2: {similarity_doc2[0][0]}\")\n",
    "print(f\"Similarity with Document 3: {similarity_doc3[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see the results are as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T08:31:15.370660Z",
     "start_time": "2024-09-16T08:31:12.351003Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity with Document 1 (AI in Healthcare): 0.7179515957832336\n",
      "Similarity with Document 2 (Cloud Computing): 0.5887858271598816\n",
      "Similarity with Document 3 (Neural Networks in AI): 0.6379864811897278\n",
      "Similarity with Document 4 (Sustainable Energy): 0.6569733023643494\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to get the [CLS] embedding for a given sentence\n",
    "def get_cls_embedding(sentence):\n",
    "    # Tokenize the sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "    \n",
    "    # Get the embeddings from the BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the [CLS] token embedding (first token)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "    return cls_embedding\n",
    "\n",
    "# Example documents (longer text)\n",
    "doc1 = \"\"\"\n",
    "Artificial Intelligence (AI) has transformed the healthcare industry by offering new ways to diagnose, \n",
    "treat, and manage diseases. AI algorithms, particularly deep learning, are being used to analyze medical \n",
    "images, predict disease outbreaks, and personalize treatment plans. The integration of AI in healthcare has \n",
    "reduced human error, improved accuracy, and increased the efficiency of medical professionals. AI-powered \n",
    "robots are assisting surgeons in complex procedures, while predictive analytics is helping doctors make \n",
    "more informed decisions. Despite these advances, there are challenges such as data privacy and the need \n",
    "for comprehensive validation of AI models before widespread adoption.\n",
    "\"\"\"\n",
    "\n",
    "doc2 = \"\"\"\n",
    "Cloud computing has revolutionized the way businesses operate, offering flexible and scalable infrastructure \n",
    "that can adjust to their needs. By moving to the cloud, companies no longer need to invest heavily in \n",
    "on-premise hardware. Instead, they can access powerful computing resources over the internet, enabling them \n",
    "to focus on innovation and growth. Businesses use cloud services for data storage, application hosting, and \n",
    "collaboration, benefiting from reduced costs, enhanced security, and improved accessibility. However, concerns \n",
    "about data breaches and vendor lock-in persist, as companies need to carefully select cloud providers to \n",
    "ensure long-term sustainability.\n",
    "\"\"\"\n",
    "\n",
    "doc3 = \"\"\"\n",
    "Neural networks, a fundamental building block of artificial intelligence, have evolved significantly since their \n",
    "inception. Initially inspired by the human brain, neural networks are designed to mimic the way neurons in \n",
    "the brain process information. Over the years, advances in deep learning, a subset of neural networks, have made \n",
    "it possible for AI systems to achieve unprecedented levels of accuracy in tasks like image recognition, natural \n",
    "language processing, and autonomous driving. Neural networks are composed of layers of interconnected nodes, \n",
    "where each node represents a neuron. The training of neural networks involves adjusting weights based on input \n",
    "data, allowing the model to learn patterns and make predictions. Despite their success, training large neural \n",
    "networks requires significant computational power and data.\n",
    "\"\"\"\n",
    "\n",
    "doc4 = \"\"\"\n",
    "As the world faces the growing threat of climate change, sustainable energy has become a major focus of global \n",
    "efforts. Renewable energy sources such as solar, wind, and hydropower are being developed to reduce dependence \n",
    "on fossil fuels. Clean technologies are playing a critical role in achieving sustainability goals, with innovations \n",
    "in energy storage, electric vehicles, and smart grids leading the way. Governments and private companies alike are \n",
    "investing heavily in research and development to create more efficient and cost-effective solutions. While the \n",
    "transition to sustainable energy presents challenges, including the initial cost of infrastructure and the need for \n",
    "reliable energy storage, it also offers immense benefits in terms of reducing greenhouse gas emissions and creating \n",
    "new economic opportunities.\n",
    "\"\"\"\n",
    "\n",
    "# Query to compare with the documents\n",
    "query = \"How do neural networks function in artificial intelligence, and what are the challenges of training them?\"\n",
    "\n",
    "# Get embeddings for documents and query\n",
    "doc_embedding1 = get_cls_embedding(doc1)\n",
    "doc_embedding2 = get_cls_embedding(doc2)\n",
    "doc_embedding3 = get_cls_embedding(doc3)\n",
    "doc_embedding4 = get_cls_embedding(doc4)\n",
    "query_embedding = get_cls_embedding(query)\n",
    "\n",
    "# Compute cosine similarities between the query and each document\n",
    "similarity_doc1 = cosine_similarity(query_embedding, doc_embedding1)\n",
    "similarity_doc2 = cosine_similarity(query_embedding, doc_embedding2)\n",
    "similarity_doc3 = cosine_similarity(query_embedding, doc_embedding3)\n",
    "similarity_doc4 = cosine_similarity(query_embedding, doc_embedding4)\n",
    "\n",
    "# Show similarity results\n",
    "print(f\"Similarity with Document 1 (AI in Healthcare): {similarity_doc1[0][0]}\")\n",
    "print(f\"Similarity with Document 2 (Cloud Computing): {similarity_doc2[0][0]}\")\n",
    "print(f\"Similarity with Document 3 (Neural Networks in AI): {similarity_doc3[0][0]}\")\n",
    "print(f\"Similarity with Document 4 (Sustainable Energy): {similarity_doc4[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the issue with BERT was he computed based on the word and which had the most occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T08:31:15.379654Z",
     "start_time": "2024-09-16T08:31:15.373658Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
