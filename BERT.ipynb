{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load BERT and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Video Processing: Object Detection and Audio Transcription"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook, we will process several video clips to:\n",
    "\n",
    "- Perform object detection using different models.\n",
    "- Transcribe audio using different transcription tools.\n",
    "- Measure performance metrics such as processing speed, detection accuracy, and transcription quality.\n",
    "- Compare the performance of different tools.\n",
    "\n",
    "**Tools Used:**\n",
    "\n",
    "- **Object Detection Models**:\n",
    "  - YOLOv8 (Ultralytics)\n",
    "  - YOLOv5 (Ultralytics) \n",
    "  - BLIP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Install necessary packages for document parsing, YOLO object detection, BLIP and data analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Install Ultralytics for YOLOv8 and YOLOv5\n",
    "!pip install ultralytics\n",
    "\n",
    "# Install OpenCV for video processing\n",
    "!pip install opencv-python\n",
    "\n",
    "# Install pandas for data manipulation\n",
    "!pip install pandas\n",
    "\n",
    "# Install matplotlib and seaborn for visualization\n",
    "!pip install matplotlib seaborn\n",
    "\n",
    "# Install PyTorch and related libraries for deep learning\n",
    "!pip install torch torchvision torchaudio\n",
    "\n",
    "# Install Hugging Face Transformers for NLP and computer vision models\n",
    "!pip install transformers\n",
    "\n",
    "# Install Hugging Face Datasets for data handling\n",
    "!pip install datasets\n",
    "\n",
    "# Install tqdm for progress bars\n",
    "!pip install tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import necessary libraries and AI models for , object detection, video processing, and data analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data for analysing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# List of video files\n",
    "video_files = [\n",
    "    'fruit-and-vegetable-detection.mp4',  # High resolution, good lighting\n",
    "    'traffic-mini.mp4',  # Medium resolution, low lighting\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function to extract frames from the video"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_frames(video_path, frame_interval=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    success, frame = cap.read()\n",
    "    count = 0\n",
    "\n",
    "    while success:\n",
    "        if count % frame_interval == 0:\n",
    "            frames.append(frame)\n",
    "        success, frame = cap.read()\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "# Example usage\n",
    "video_path = 'fruit-and-vegetable-detection.mp4'  # Update with your video path\n",
    "frames = extract_frames(video_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the BLIP processor and model for Video Question Answering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to perform Video Question Answering using BLIP\n",
    "def blip_video_qa(frames, question):\n",
    "    answers = []\n",
    "\n",
    "    for i, frame in enumerate(frames):\n",
    "        # Convert frame to PIL Image\n",
    "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        # Prepare inputs for the model\n",
    "        inputs = processor(image, question, return_tensors=\"pt\")\n",
    "\n",
    "        # Generate the answer\n",
    "        output = model.generate(**inputs)\n",
    "        answer = processor.decode(output[0], skip_special_tokens=True)\n",
    "        answers.append((i, answer))  # Store frame index and answer\n",
    "\n",
    "        print(f\"Frame {i} Answer: {answer}\")\n",
    "\n",
    "    return answers\n",
    "\n",
    "# Define your question\n",
    "question = \"When in the video does it show the apple for the first time?\"\n",
    "\n",
    "# Perform Video QA using BLIP\n",
    "blip_answers = blip_video_qa(frames, question)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Measure processing time for BLIP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "blip_answers = blip_video_qa(frames, question)\n",
    "blip_processing_time = time.time() - start_time\n",
    "\n",
    "print(f\"BLIP Processing Time: {blip_processing_time:.2f} seconds\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function for Object Detection in Videos Using YOLOv8"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def yolo_v8_detection(video_path, user_object, frame_interval=5):\n",
    "    model = YOLO('yolov8s.pt')  # Load the YOLOv8 model\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = 0\n",
    "    timestamps = []\n",
    "    processing_times = []\n",
    "    class_names = model.names\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "\n",
    "        if frame_count % frame_interval != 0:\n",
    "            continue\n",
    "\n",
    "        start_time = time.time()\n",
    "        results = model(frame)\n",
    "        end_time = time.time()\n",
    "        processing_times.append(end_time - start_time)\n",
    "\n",
    "        for result in results:\n",
    "            boxes = result.boxes\n",
    "            for box in boxes:\n",
    "                class_id = int(box.cls[0])\n",
    "                class_name = class_names[class_id]\n",
    "                if class_name == user_object:\n",
    "                    timestamp = frame_count / fps\n",
    "                    timestamps.append(timestamp)\n",
    "                    break\n",
    "\n",
    "    cap.release()\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "\n",
    "    return {\n",
    "        'timestamps': sorted(set(timestamps)),\n",
    "        'processing_time': avg_processing_time,\n",
    "        'frames_processed': frame_count // frame_interval\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function for Object Detection in Videos Using YOLOv5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def yolo_v5_detection(video_path, user_object, frame_interval=5):\n",
    "    model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = 0\n",
    "    timestamps = []\n",
    "    processing_times = []\n",
    "    class_names = model.names\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "\n",
    "        if frame_count % frame_interval != 0:\n",
    "            continue\n",
    "\n",
    "        start_time = time.time()\n",
    "        results = model(frame)\n",
    "        end_time = time.time()\n",
    "        processing_times.append(end_time - start_time)\n",
    "\n",
    "        # Parse detection results\n",
    "        labels = results.xyxy[0][:, -1].cpu().numpy()\n",
    "        for label in labels:\n",
    "            class_name = class_names[int(label)]\n",
    "            if class_name == user_object:\n",
    "                timestamp = frame_count / fps\n",
    "                timestamps.append(timestamp)\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "\n",
    "    return {\n",
    "        'timestamps': sorted(set(timestamps)),\n",
    "        'processing_time': avg_processing_time,\n",
    "        'frames_processed': frame_count // frame_interval\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Script for Running Object Detection and Video Question Answering with YOLOv8, YOLOv5, and BLIP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "video_user_objects = {\n",
    "    'fruit-and-vegetable-detection.mp4': 'apple',\n",
    "    'traffic-mini.mp4': 'truck',\n",
    "    # Add more mappings if needed\n",
    "}\n",
    "# List to store results\n",
    "results_list = []\n",
    "\n",
    "for video in video_files:\n",
    "    print(f\"Processing {video}...\")\n",
    "    \n",
    "    video_basename = os.path.basename(video)\n",
    "    user_object = video_user_objects.get(video_basename, None)\n",
    "    if user_object is None:\n",
    "        print(f\"No user_object specified for {video_basename}. Skipping this video.\")\n",
    "        continue\n",
    "    \n",
    "    # YOLOv8 Detection\n",
    "    yolo_v8_results = yolo_v8_detection(video, user_object)\n",
    "    \n",
    "    \n",
    "    results_list.append({\n",
    "        'Video': video_basename,\n",
    "        'Tool': 'YOLOv8',\n",
    "        'Processing Time (s)': yolo_v8_results['processing_time'],\n",
    "        'Frames Processed': yolo_v8_results['frames_processed']\n",
    "    })\n",
    "    \n",
    "    # YOLOv5 Detection\n",
    "    yolo_v5_results = yolo_v5_detection(video, user_object)\n",
    "  \n",
    "    \n",
    "    results_list.append({\n",
    "        'Video': video_basename,\n",
    "        'Tool': 'YOLOv5',\n",
    "        'Processing Time (s)': yolo_v5_results['processing_time'],\n",
    "        'Frames Processed': yolo_v5_results['frames_processed']\n",
    "    })\n",
    "    # BLIP Detection (using Video QA)\n",
    "    start_time = time.time()\n",
    "    blip_answers = blip_video_qa(extract_frames(video), \"When in the video its show on the first time the apple?\")\n",
    "    blip_processing_time = time.time() - start_time\n",
    "\n",
    "    results_list.append({\n",
    "        'Video': video_basename,\n",
    "        'Tool': 'BLIP',\n",
    "        'Processing Time (s)': blip_processing_time,\n",
    "        'Frames Processed': len(frames)\n",
    "    })\n",
    "    results_df = pd.DataFrame(results_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Script for Displaying and Visualizing Object Detection and Video QA Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Ensure all columns are displayed\n",
    "pd.set_option('display.expand_frame_repr', False)  # Avoid line breaks in the table display\n",
    "pd.set_option('display.colheader_justify', 'center')  # Center-align column headers\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(\"\\nSummary of Processing Results:\")\n",
    "print(results_df.to_string(index=False))  # Display the DataFrame in a readable format\n",
    "\n",
    "# Plot Processing Time Comparison for YOLOv8, YOLOv5, and BLIP\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=results_df, x='Tool', y='Processing Time (s)', hue='Video')\n",
    "plt.title('Processing Time Comparison for YOLOv8, YOLOv5, and BLIP')\n",
    "plt.xlabel('Detection Tool')\n",
    "plt.ylabel('Processing Time (seconds)')\n",
    "plt.legend(title='Video File', loc='upper right')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()  # Adjust layout for better fit\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T08:31:00.792462Z",
     "start_time": "2024-09-16T08:31:00.112323Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x BertLayer(\n        (attention): BertAttention(\n          (self): BertSdpaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract embeddings from BERT using the [CLS] token, which will serve as the sentence embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T08:31:00.806545Z",
     "start_time": "2024-09-16T08:31:00.796454Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to get the [CLS] embedding for a given sentence\n",
    "def get_cls_embedding(sentence):\n",
    "    # Tokenize the sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "    \n",
    "    # Get the embeddings from the BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the [CLS] token embedding (first token)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "    return cls_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute the similarity between different queries using cosine similarity to show how BERT handles semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T08:31:07.313896Z",
     "start_time": "2024-09-16T08:31:00.809543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between query 1 and query 2: 0.8579674959182739\n",
      "Similarity between query 1 and query 3: 0.8106188774108887\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Get embeddings for similar and different queries\n",
    "query1 = \"Explain the role of backpropagation in deep learning.\"\n",
    "query2 = \"How do convolutional neural networks work?\"\n",
    "query3 = \"What is the process to register for courses at the university?\"\n",
    "\n",
    "embedding1 = get_cls_embedding(query1)\n",
    "embedding2 = get_cls_embedding(query2)\n",
    "embedding3 = get_cls_embedding(query3)\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarity_1_2 = cosine_similarity(embedding1, embedding2)\n",
    "similarity_1_3 = cosine_similarity(embedding1, embedding3)\n",
    "\n",
    "print(f\"Similarity between query 1 and query 2: {similarity_1_2[0][0]}\")\n",
    "print(f\"Similarity between query 1 and query 3: {similarity_1_3[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text retrevial using BERT we embedd the documents with a cls token which respresents the sentence and we then do a similarity check between the query .\n",
    "since the query is about backpropagtion document 1 and 3 should have a higher similarity than doc 2 which is about vpb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T08:31:10.623167Z",
     "start_time": "2024-09-16T08:31:07.320884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity with Document 1: 0.8547440767288208\n",
      "Similarity with Document 2: 0.8254550695419312\n",
      "Similarity with Document 3: 0.8463909029960632\n"
     ]
    }
   ],
   "source": [
    "# Example documents\n",
    "doc1 = \"Neural networks are computing systems inspired by the biological neural networks.\"\n",
    "doc2 = \"To connect to the university VPN, you need to configure your VPN client.\"\n",
    "doc3 = \"Backpropagation is a fundamental algorithm in training deep learning models.\"\n",
    "\n",
    "# Get embeddings for documents\n",
    "doc_embedding1 = get_cls_embedding(doc1)\n",
    "doc_embedding2 = get_cls_embedding(doc2)\n",
    "doc_embedding3 = get_cls_embedding(doc3)\n",
    "\n",
    "# Compare query to documents\n",
    "query = \"How does backpropagation work in neural networks?\"\n",
    "\n",
    "query_embedding = get_cls_embedding(query)\n",
    "\n",
    "# Compute cosine similarities between query and documents\n",
    "similarity_doc1 = cosine_similarity(query_embedding, doc_embedding1)\n",
    "similarity_doc2 = cosine_similarity(query_embedding, doc_embedding2)\n",
    "similarity_doc3 = cosine_similarity(query_embedding, doc_embedding3)\n",
    "\n",
    "# Show results\n",
    "print(f\"Similarity with Document 1: {similarity_doc1[0][0]}\")\n",
    "print(f\"Similarity with Document 2: {similarity_doc2[0][0]}\")\n",
    "print(f\"Similarity with Document 3: {similarity_doc3[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T08:31:12.348492Z",
     "start_time": "2024-09-16T08:31:10.627681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity with Document 1: 0.7179515957832336\n",
      "Similarity with Document 2: 0.5887858271598816\n",
      "Similarity with Document 3: 0.6379864811897278\n"
     ]
    }
   ],
   "source": [
    "# Example documents\n",
    "doc1 = \"Artificial Intelligence (AI) has transformed the healthcare industry by offering new ways to diagnose, treat, and manage diseases. AI algorithms, particularly deep learning, are being used to analyze medical images, predict disease outbreaks, and personalize treatment plans. The integration of AI in healthcare has reduced human error, improved accuracy, and increased the efficiency of medical professionals. AI-powered robots are assisting surgeons in complex procedures, while predictive analytics is helping doctors make more informed decisions. Despite these advances, there are challenges such as data privacy and the need for comprehensive validation of AI models before widespread adoption.\"\n",
    "doc2 = \"Cloud computing has revolutionized the way businesses operate, offering flexible and scalable infrastructure that can adjust to their needs. By moving to the cloud, companies no longer need to invest heavily in on-premise hardware. Instead, they can access powerful computing resources over the internet, enabling them to focus on innovation and growth. Businesses use cloud services for data storage, application hosting, and collaboration, benefiting from reduced costs, enhanced security, and improved accessibility. However, concerns about data breaches and vendor lock-in persist, as companies need to carefully select cloud providers to ensure long-term sustainability.\"\n",
    "doc3 = \"Neural networks, a fundamental building block of artificial intelligence, have evolved significantly since their inception. Initially inspired by the human brain, neural networks are designed to mimic the way neurons in the brain process information. Over the years, advances in deep learning, a subset of neural networks, have made it possible for AI systems to achieve unprecedented levels of accuracy in tasks like image recognition, natural language processing, and autonomous driving. Neural networks are composed of layers of interconnected nodes, where each node represents a neuron. The training of neural networks involves adjusting weights based on input data, allowing the model to learn patterns and make predictions. Despite their success, training large neural networks requires significant computational power and data.\"\n",
    "\n",
    "# Get embeddings for documents\n",
    "doc_embedding1 = get_cls_embedding(doc1)\n",
    "doc_embedding2 = get_cls_embedding(doc2)\n",
    "doc_embedding3 = get_cls_embedding(doc3)\n",
    "\n",
    "# Compare query to documents\n",
    "query = \"How do neural networks function in artificial intelligence, and what are the challenges of training them?\"\n",
    "\n",
    "query_embedding = get_cls_embedding(query)\n",
    "\n",
    "# Compute cosine similarities between query and documents\n",
    "similarity_doc1 = cosine_similarity(query_embedding, doc_embedding1)\n",
    "similarity_doc2 = cosine_similarity(query_embedding, doc_embedding2)\n",
    "similarity_doc3 = cosine_similarity(query_embedding, doc_embedding3)\n",
    "\n",
    "# Show results\n",
    "print(f\"Similarity with Document 1: {similarity_doc1[0][0]}\")\n",
    "print(f\"Similarity with Document 2: {similarity_doc2[0][0]}\")\n",
    "print(f\"Similarity with Document 3: {similarity_doc3[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see the results are as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T08:31:15.370660Z",
     "start_time": "2024-09-16T08:31:12.351003Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity with Document 1 (AI in Healthcare): 0.7179515957832336\n",
      "Similarity with Document 2 (Cloud Computing): 0.5887858271598816\n",
      "Similarity with Document 3 (Neural Networks in AI): 0.6379864811897278\n",
      "Similarity with Document 4 (Sustainable Energy): 0.6569733023643494\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to get the [CLS] embedding for a given sentence\n",
    "def get_cls_embedding(sentence):\n",
    "    # Tokenize the sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "    \n",
    "    # Get the embeddings from the BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the [CLS] token embedding (first token)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "    return cls_embedding\n",
    "\n",
    "# Example documents (longer text)\n",
    "doc1 = \"\"\"\n",
    "Artificial Intelligence (AI) has transformed the healthcare industry by offering new ways to diagnose, \n",
    "treat, and manage diseases. AI algorithms, particularly deep learning, are being used to analyze medical \n",
    "images, predict disease outbreaks, and personalize treatment plans. The integration of AI in healthcare has \n",
    "reduced human error, improved accuracy, and increased the efficiency of medical professionals. AI-powered \n",
    "robots are assisting surgeons in complex procedures, while predictive analytics is helping doctors make \n",
    "more informed decisions. Despite these advances, there are challenges such as data privacy and the need \n",
    "for comprehensive validation of AI models before widespread adoption.\n",
    "\"\"\"\n",
    "\n",
    "doc2 = \"\"\"\n",
    "Cloud computing has revolutionized the way businesses operate, offering flexible and scalable infrastructure \n",
    "that can adjust to their needs. By moving to the cloud, companies no longer need to invest heavily in \n",
    "on-premise hardware. Instead, they can access powerful computing resources over the internet, enabling them \n",
    "to focus on innovation and growth. Businesses use cloud services for data storage, application hosting, and \n",
    "collaboration, benefiting from reduced costs, enhanced security, and improved accessibility. However, concerns \n",
    "about data breaches and vendor lock-in persist, as companies need to carefully select cloud providers to \n",
    "ensure long-term sustainability.\n",
    "\"\"\"\n",
    "\n",
    "doc3 = \"\"\"\n",
    "Neural networks, a fundamental building block of artificial intelligence, have evolved significantly since their \n",
    "inception. Initially inspired by the human brain, neural networks are designed to mimic the way neurons in \n",
    "the brain process information. Over the years, advances in deep learning, a subset of neural networks, have made \n",
    "it possible for AI systems to achieve unprecedented levels of accuracy in tasks like image recognition, natural \n",
    "language processing, and autonomous driving. Neural networks are composed of layers of interconnected nodes, \n",
    "where each node represents a neuron. The training of neural networks involves adjusting weights based on input \n",
    "data, allowing the model to learn patterns and make predictions. Despite their success, training large neural \n",
    "networks requires significant computational power and data.\n",
    "\"\"\"\n",
    "\n",
    "doc4 = \"\"\"\n",
    "As the world faces the growing threat of climate change, sustainable energy has become a major focus of global \n",
    "efforts. Renewable energy sources such as solar, wind, and hydropower are being developed to reduce dependence \n",
    "on fossil fuels. Clean technologies are playing a critical role in achieving sustainability goals, with innovations \n",
    "in energy storage, electric vehicles, and smart grids leading the way. Governments and private companies alike are \n",
    "investing heavily in research and development to create more efficient and cost-effective solutions. While the \n",
    "transition to sustainable energy presents challenges, including the initial cost of infrastructure and the need for \n",
    "reliable energy storage, it also offers immense benefits in terms of reducing greenhouse gas emissions and creating \n",
    "new economic opportunities.\n",
    "\"\"\"\n",
    "\n",
    "# Query to compare with the documents\n",
    "query = \"How do neural networks function in artificial intelligence, and what are the challenges of training them?\"\n",
    "\n",
    "# Get embeddings for documents and query\n",
    "doc_embedding1 = get_cls_embedding(doc1)\n",
    "doc_embedding2 = get_cls_embedding(doc2)\n",
    "doc_embedding3 = get_cls_embedding(doc3)\n",
    "doc_embedding4 = get_cls_embedding(doc4)\n",
    "query_embedding = get_cls_embedding(query)\n",
    "\n",
    "# Compute cosine similarities between the query and each document\n",
    "similarity_doc1 = cosine_similarity(query_embedding, doc_embedding1)\n",
    "similarity_doc2 = cosine_similarity(query_embedding, doc_embedding2)\n",
    "similarity_doc3 = cosine_similarity(query_embedding, doc_embedding3)\n",
    "similarity_doc4 = cosine_similarity(query_embedding, doc_embedding4)\n",
    "\n",
    "# Show similarity results\n",
    "print(f\"Similarity with Document 1 (AI in Healthcare): {similarity_doc1[0][0]}\")\n",
    "print(f\"Similarity with Document 2 (Cloud Computing): {similarity_doc2[0][0]}\")\n",
    "print(f\"Similarity with Document 3 (Neural Networks in AI): {similarity_doc3[0][0]}\")\n",
    "print(f\"Similarity with Document 4 (Sustainable Energy): {similarity_doc4[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the issue with BERT was he computed based on the word and which had the most occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T08:31:15.379654Z",
     "start_time": "2024-09-16T08:31:15.373658Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
