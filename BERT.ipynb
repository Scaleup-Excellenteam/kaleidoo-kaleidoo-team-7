{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmqTDbVP_GIg"
      },
      "source": [
        "# **Research**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQUnlOHx_hxZ"
      },
      "source": [
        "## **Input parsing:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl4w89d6Bv8v"
      },
      "source": [
        "### **Step 1: Audio Transcription**\n",
        "\n",
        "1.   Installing the `faster_whisper` Library\n",
        "2.   Importing Necessary Libraries\n",
        "3.   Determining the Computing Device\n",
        "4.   Selecting the Model Size and Compute Type\n",
        "5.   Loading the Whisper Model\n",
        "6.   Transcribing the Audio File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL06qyByBHZY"
      },
      "outputs": [],
      "source": [
        "pip install faster_whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVSGlJd3BU18"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from faster_whisper import WhisperModel\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model_size = \"small\"\n",
        "\n",
        "if device == \"cpu\":\n",
        "    compute_type = \"int8\"\n",
        "else:\n",
        "    compute_type = \"float16\"\n",
        "\n",
        "\n",
        "model = WhisperModel(model_size, device=device, compute_type=compute_type)\n",
        "\n",
        "audio_file = './america.wav'\n",
        "\n",
        "segments, info = model.transcribe(audio_file, beam_size=1)\n",
        "\n",
        "transcription = \"\"\n",
        "for segment in segments:\n",
        "    transcription += segment.text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Documents Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y tesseract-ocr\n",
        "!sudo apt-get install -y tesseract-ocr-heb\n",
        "# Install Python packages\n",
        "!pip install pytesseract\n",
        "!pip install langdetect\n",
        "!pip install Pillow\n",
        "!pip install pandas\n",
        "!pip install PyPDF2\n",
        "!pip install python-pptx\n",
        "!pip install python-docx\n",
        "!pip install pdfminer.six\n",
        "\n",
        "# Verify installed languages\n",
        "!tesseract --list-langs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import LIB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pytesseract\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "from google.colab import drive\n",
        "import re\n",
        "from langdetect import detect_langs, DetectorFactory\n",
        "from pptx import Presentation\n",
        "from docx import Document\n",
        "from pdfminer.high_level import extract_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "taking all the languages that we need to work in, lang_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure consistent language detection\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "# Extended language map (detect to Tesseract)\n",
        "lang_map = {\n",
        "    'en': 'eng',    # English\n",
        "    'es': 'spa',    # Spanish\n",
        "    'fr': 'fra',    # French\n",
        "    'de': 'deu',    # German\n",
        "    'it': 'ita',    # Italian\n",
        "    'pt': 'por',    # Portuguese\n",
        "    'ru': 'rus',    # Russian\n",
        "    'zh-cn': 'chi_sim',  # Simplified Chinese\n",
        "    'zh-tw': 'chi_tra',  # Traditional Chinese\n",
        "    'ja': 'jpn',    # Japanese\n",
        "    'ko': 'kor',    # Korean\n",
        "    'ar': 'ara',    # Arabic\n",
        "    'he': 'heb',    # Hebrew\n",
        "    'fa': 'fas',    # Persian (Farsi)\n",
        "    'hi': 'hin',    # Hindi\n",
        "    'th': 'tha',    # Thai\n",
        "    'vi': 'vie',    # Vietnamese\n",
        "    'nl': 'nld',    # Dutch\n",
        "    'tr': 'tur',    # Turkish\n",
        "    'pl': 'pol',    # Polish\n",
        "    'uk': 'ukr',    # Ukrainian\n",
        "    'ro': 'ron',    # Romanian\n",
        "    'bg': 'bul',    # Bulgarian\n",
        "    'el': 'ell',    # Greek\n",
        "    'ur': 'urd',    # Urdu\n",
        "    # Add more languages as needed\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract txt from txt, PDF, docs, pptx files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_txt_file(file_path, encoding='utf-8'):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding=encoding) as file:\n",
        "            lines = [line.rstrip('\\n') for line in file]\n",
        "        return lines\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File {file_path} not found.\")\n",
        "    except IOError as e:\n",
        "        print(f\"An I/O error occurred: {e}\")\n",
        "    return []\n",
        "\n",
        "def read_pdf_with_pdfminer(file_path):\n",
        "    try:\n",
        "        text = extract_text(file_path)\n",
        "        lines = text.splitlines()\n",
        "        return lines\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File {file_path} not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "    return []\n",
        "\n",
        "def read_pptx_file(file_path):\n",
        "    try:\n",
        "        prs = Presentation(file_path)\n",
        "        text_runs = []\n",
        "        for slide in prs.slides:\n",
        "            for shape in slide.shapes:\n",
        "                if hasattr(shape, \"text\"):\n",
        "                    text_runs.append(shape.text)\n",
        "        return text_runs\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File {file_path} not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "    return []\n",
        "\n",
        "def read_docx_file(file_path):\n",
        "    try:\n",
        "        doc = Document(file_path)\n",
        "        text = []\n",
        "        for para in doc.paragraphs:\n",
        "            text.append(para.text)\n",
        "        return text\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File {file_path} not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "    return []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract txt from image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_image(image_path):\n",
        "    \"\"\"\n",
        "    Preprocesses an image to improve OCR accuracy.\n",
        "\n",
        "    Steps:\n",
        "    - Convert to grayscale\n",
        "    - Apply median filter for noise reduction\n",
        "    - Enhance contrast\n",
        "    - Binarize the image using thresholding\n",
        "    \"\"\"\n",
        "    try:\n",
        "        image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
        "        image = image.filter(ImageFilter.MedianFilter())  # Reduce noise\n",
        "        enhancer = ImageEnhance.Contrast(image)\n",
        "        image = enhancer.enhance(2)  # Enhance contrast\n",
        "        image = image.point(lambda x: 0 if x < 140 else 255, '1')  # Binarization\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_text_multiple_languages(image, languages=['eng', 'heb']):\n",
        "    \"\"\"\n",
        "    Extracts text from an image using Tesseract OCR with multiple languages.\n",
        "\n",
        "    Args:\n",
        "        image (PIL.Image): Preprocessed image.\n",
        "        languages (list): List of language codes to use for OCR.\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Join language codes with '+' for Tesseract\n",
        "        lang_param = '+'.join(languages)\n",
        "        text = pytesseract.image_to_string(image, lang=lang_param)\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during OCR with multiple languages: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "\n",
        "def further_clean_text(text):\n",
        "    \"\"\"\n",
        "    Further cleans the extracted text by removing unwanted characters.\n",
        "\n",
        "    Args:\n",
        "        text (str): Extracted text.\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned text.\n",
        "    \"\"\"\n",
        "    # Remove non-printable characters\n",
        "    text = ''.join(filter(lambda x: x.isprintable(), text))\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Supported image formats\n",
        "image_extensions = ['*.png', '*.jpg', '*.jpeg', '*.tiff', '*.bmp']\n",
        "\n",
        "# Gather all image file paths\n",
        "image_paths = ['/content/Screenshot 2024-09-16 at 11.59.44.png']\n",
        "\n",
        "\n",
        "print(f\"Found {len(image_paths)} images.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define what the file type is to extraxt the txt from it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize lists to store results\n",
        "extracted_text = []\n",
        "detected_file_types = []\n",
        "file_names = []\n",
        "\n",
        "# Define desired languages for OCR\n",
        "desired_languages = ['eng', 'heb', 'spa']  # Add more as needed\n",
        "\n",
        "def extract_text_from_file(file_path, use_pdfminer=False):\n",
        "    _, file_extension = os.path.splitext(file_path)\n",
        "    file_extension = file_extension.lower()\n",
        "\n",
        "    if file_extension == '.txt':\n",
        "        text = read_txt_file(file_path)\n",
        "        return text, \"txt\"\n",
        "\n",
        "    elif file_extension == '.pdf':\n",
        "        if use_pdfminer:\n",
        "            text = read_pdf_with_pdfminer(file_path)\n",
        "        else:\n",
        "            # If you have another PDF reader, integrate it here\n",
        "            text = read_pdf_with_pdfminer(file_path)\n",
        "        return text, \"pdf\"\n",
        "\n",
        "    elif file_extension == '.pptx':\n",
        "        text = read_pptx_file(file_path)\n",
        "        return text, \"pptx\"\n",
        "\n",
        "    elif file_extension == '.docx':\n",
        "        text = read_docx_file(file_path)\n",
        "        return text, \"docx\"\n",
        "\n",
        "    elif file_extension in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']:\n",
        "        # Process image with OCR\n",
        "        preprocessed_img = preprocess_image(file_path)\n",
        "        if preprocessed_img is None:\n",
        "            return \"\", \"preprocessing_failed\"\n",
        "        text = extract_text_multiple_languages(preprocessed_img, languages=desired_languages)\n",
        "        return text, \"image\"\n",
        "\n",
        "    else:\n",
        "        print(f\"Unsupported file type: {file_extension}\")\n",
        "        return \"\", \"unsupported\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The main part taking the file path save it on uploaded list and extract all the files from there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your list of files to process\n",
        "uploaded = ['/content/Screenshot 2024-09-16 at 11.59.44.png','/content/תשובות.docx','/content/Full Stack Project.pdf']\n",
        "\n",
        "# Process all uploaded files\n",
        "for idx, filename in enumerate(uploaded):\n",
        "    print(f\"Processing File {idx + 1}/{len(uploaded)}: {filename}\")\n",
        "    try:\n",
        "        text, file_type = extract_text_from_file(filename, use_pdfminer=True)\n",
        "        if text:\n",
        "            # For text files, text may be a list of lines; for images, text is a string\n",
        "            if isinstance(text, list):\n",
        "                cleaned_text = \"\\n\".join([further_clean_text(line) for line in text])\n",
        "            else:\n",
        "                cleaned_text = further_clean_text(text)\n",
        "            extracted_text.append(cleaned_text)\n",
        "            detected_file_types.append(file_type)\n",
        "            file_names.append(os.path.basename(filename))\n",
        "\n",
        "            # Print a preview of the extracted text\n",
        "            preview = cleaned_text[:100].replace('\\n', ' ') + ('...' if len(cleaned_text) > 100 else '')\n",
        "            print(cleaned_text)\n",
        "\n",
        "            print(f\"Extracted Text Preview: {preview}\")\n",
        "        else:\n",
        "            print(\"No text extracted.\")\n",
        "            extracted_text.append(\"\")\n",
        "            detected_file_types.append(\"no_text_extracted\")\n",
        "            file_names.append(os.path.basename(filename))\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {filename}: {e}\")\n",
        "        extracted_text.append(\"\")\n",
        "        detected_file_types.append(\"error\")\n",
        "        file_names.append(os.path.basename(filename))\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnjCscl_CHOf"
      },
      "source": [
        "## **Retrieval-Augmented Generation (RAG)**\n",
        "\n",
        "This script is designed to identify and retrieve the most relevant documents from a predefined corpus based on a user-provided query. It leverages the power of Sentence Transformers to convert text into meaningful vector embeddings and uses cosine similarity from scikit-learn to measure the semantic similarity between the query and the documents. Additionally, the script includes multilingual support by incorporating documents in Hebrew."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd085ytHCM9d"
      },
      "source": [
        "### **Step 1: Text Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTLx9nqdDaHp"
      },
      "source": [
        "1.   Installing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQz4hcOVCT3U",
        "outputId": "27b10aef-fbcb-4c2c-8e5c-178e844cbd85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install sentence_transformers scikit-learn transformers torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59E_-we0Dgrv"
      },
      "source": [
        "2.    Importing Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLUmKNpSDkHw",
        "outputId": "85d1c329-7acd-4508-e71d-096908da1127"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import time\n",
        "import numpy as np\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh469QLUDm9x"
      },
      "source": [
        "3.   Loading the Sentence Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmamxziFDrQ9",
        "outputId": "8ff1a312-83c8-45b6-a744-3bf97296a6f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU4h5E0RDtJP"
      },
      "source": [
        "4. Defining the Corpus of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbUytWAADyTq"
      },
      "outputs": [],
      "source": [
        "data =[ \"\"\"\n",
        "    Artificial Intelligence (AI) has transformed the healthcare industry by offering new ways to diagnose,\n",
        "    treat, and manage diseases. AI algorithms, particularly deep learning, are being used to analyze medical\n",
        "    images, predict disease outbreaks, and personalize treatment plans. The integration of AI in healthcare has\n",
        "    reduced human error, improved accuracy, and increased the efficiency of medical professionals. AI-powered\n",
        "    robots are assisting surgeons in complex procedures, while predictive analytics is helping doctors make\n",
        "    more informed decisions. Despite these advances, there are challenges such as data privacy and the need\n",
        "    for comprehensive validation of AI models before widespread adoption.\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    Cloud computing has revolutionized the way businesses operate, offering flexible and scalable infrastructure\n",
        "    that can adjust to their needs. By moving to the cloud, companies no longer need to invest heavily in\n",
        "    on-premise hardware. Instead, they can access powerful computing resources over the internet, enabling them\n",
        "    to focus on innovation and growth. Businesses use cloud services for data storage, application hosting, and\n",
        "    collaboration, benefiting from reduced costs, enhanced security, and improved accessibility. However, concerns\n",
        "    about data breaches and vendor lock-in persist, as companies need to carefully select cloud providers to\n",
        "    ensure long-term sustainability.\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    Neural networks, a fundamental building block of artificial intelligence, have evolved significantly since their\n",
        "    inception. Initially inspired by the human brain, neural networks are designed to mimic the way neurons in\n",
        "    the brain process information. Over the years, advances in deep learning, a subset of neural networks, have made\n",
        "    it possible for AI systems to achieve unprecedented levels of accuracy in tasks like image recognition, natural\n",
        "    language processing, and autonomous driving. Neural networks are composed of layers of interconnected nodes,\n",
        "    where each node represents a neuron. The training of neural networks involves adjusting weights based on input\n",
        "    data, allowing the model to learn patterns and make predictions. Despite their success, training large neural\n",
        "    networks requires significant computational power and data.\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    As the world faces the growing threat of climate change, sustainable energy has become a major focus of global\n",
        "    efforts. Renewable energy sources such as solar, wind, and hydropower are being developed to reduce dependence\n",
        "    on fossil fuels. Clean technologies are playing a critical role in achieving sustainability goals, with innovations\n",
        "    in energy storage, electric vehicles, and smart grids leading the way. Governments and private companies alike are\n",
        "    investing heavily in research and development to create more efficient and cost-effective solutions. While the\n",
        "    transition to sustainable energy presents challenges, including the initial cost of infrastructure and the need for\n",
        "    reliable energy storage, it also offers immense benefits in terms of reducing greenhouse gas emissions and creating\n",
        "    new economic opportunities.\n",
        "    \"\"\",\n",
        "     \"\"\"\n",
        "    מאמר 1: השפעת הבינה המלאכותית על תהליכים משפטיים\n",
        "בעשור האחרון, הבינה המלאכותית (AI) הפכה להיות כלי מרכזי בתהליכים משפטיים, בין אם במתן ייעוץ משפטי אוטומטי ובין אם בניתוח מסמכים משפטיים. אחד היתרונות המרכזיים של AI הוא היכולת לנתח כמויות גדולות של מידע במהירות וביעילות. מערכות בינה מלאכותית מסוגלות לעבד חומרים משפטיים מורכבים, כגון חוזים, תיקים משפטיים, ופסיקות עבר, ולספק תחזיות על בסיס נתונים היסטוריים. שימוש ב-AI מבטיח יעילות משופרת, חיסכון בזמן והפחתת טעויות אנוש בתהליכי קבלת החלטות. עם זאת, השימוש הגובר ב-AI מעלה שאלות אתיות ומשפטיות רבות. לדוגמה, נושאים של אחריות משפטית: האם ניתן לסמוך על מערכת אוטומטית בקבלת החלטות משפטיות? שאלות אלו מתחדדות בעיקר בתחום הפלילי, כאשר טכנולוגיות אלו משמשות ככלי לסיוע בקביעת עונשים או החלטות שיפוטיות אחרות. אחת מהשאלות המרכזיות הנוגעות ל-AI בתחום המשפט היא האם ניתן להחליט על סוגיות משפטיות בהתבסס על ניתוח של אלגוריתם בלבד, או שיש צורך באישוש ההחלטות על ידי בני אדם. כמו כן, בעידן של רגולציה טכנולוגית מוגברת, חשוב לשאול האם המערכות הללו עומדות בקריטריונים של שקיפות והוגנות, והאם ישנם מנגנונים שימנעו מהן לבצע הטיות סמויות בהחלטות שהן מקבלות. יתר על כן, השימוש ב-AI בחוזים מסחריים הפך להיות שכיח ביותר, כאשר מערכות אוטומטיות יכולות לסייע בניתוח וכתיבה של חוזים. עם זאת, יש צורך בהתאמת החקיקה כדי להבטיח שהחוזים שנוצרים על ידי מערכות אלו יעמדו בדרישות החוק. בתי המשפט נדרשים להתמודד עם פסיקות הנוגעות לחוזים שנוצרו או נחתמו באמצעות AI, וחקיקה שתסדיר נושא זה עדיין מתהווה.\n",
        "      \"\"\",\n",
        "      \"\"\"\n",
        "    מאמר 2: משפט בינלאומי וזכויות אדם בעידן הגלובליזציה\n",
        "    המשפט הבינלאומי מתמודד כיום עם אתגרים חדשים הנובעים מהתהליך המואץ של גלובליזציה והשפעתה על זכויות אדם. בעידן של תנועת אנשים, הון, וסחורות בצורה חסרת תקדים, יש צורך בהסדרים משפטיים בינלאומיים המבטיחים הגנה על זכויות האדם בכל המדינות. אמנות בינלאומיות, כגון האמנה הבינלאומית לזכויות האדם, נועדו להבטיח שכל אדם יקבל את ההגנות הנדרשות גם כאשר הוא חוצה גבולות. עם זאת, במקרים רבים נתקלים במצבים בהם יש פער בין הרטוריקה הבינלאומית לזכויות האדם לבין היישום המעשי שלהן. לדוגמה, נושא של פליטים ומהגרים הוא נושא קריטי במשפט הבינלאומי. בעוד שהאמנה לזכויות האדם מבטיחה הגנה לכל אדם, המציאות היא שמדינות רבות מוצאות דרכים לעקוף את ההתחייבויות הבינלאומיות שלהן בנוגע לפליטים, בשל שיקולים כלכליים ופוליטיים. סוגיה נוספת שעולה היא השימוש בטכנולוגיות חדשות בתחום הביטחון והמעקב, כמו מערכות זיהוי פנים ואלגוריתמים למעקב אחר תנועות אנשים. בעוד שהשימוש בטכנולוגיות אלו נועד להגביר את הביטחון, הוא מעלה שאלות אתיות ומשפטיות בנוגע לפגיעה בפרטיות וזכויות אדם. יתר על כן, גם בתחום הכלכלה הגלובלית, נדרשת התייחסות משפטית מיוחדת לנושאים של צדק חברתי ושוויון. תאגידים בינלאומיים, שפועלים במספר מדינות, מתמודדים עם בעיות משפטיות מורכבות הקשורות לזכויות עובדים, הגנה על הסביבה, ומניעת תחרות לא הוגנת. המשפט הבינלאומי נדרש להסדיר תחומים אלו ולהבטיח שהגלובליזציה לא תפגע בזכויות האדם הבסיסיות.\n",
        "    \"\"\",\n",
        "      \"\"\"\n",
        "    מאמר 3: הגנת הפרטיות בעידן הדיגיטלי\n",
        "    הגנת הפרטיות הפכה לאחד הנושאים המשפטיים החשובים ביותר בעידן הדיגיטלי. הטכנולוגיות החדשות, ובמיוחד השימוש באינטרנט ובמדיה החברתית, יצרו מצבים חדשים בהם מידע אישי זורם בצורה מהירה וחסרת גבולות, ולעיתים רבות ללא ידיעת המשתמשים. חברות ענק כמו פייסבוק, גוגל ואמזון אוספות כמויות עצומות של מידע על המשתמשים שלהן, החל מהרגלי גלישה ועד להעדפות צרכניות אישיות. המידע שנאסף מנוצל לעיתים לצרכים מסחריים, כגון פרסום ממוקד, אך הוא גם עשוי להגיע לידי צדדים שלישיים או ממשלות, מה שמעלה את החשש לפגיעה בפרטיות האישית. בעולם בו המידע האישי הפך לנכס כלכלי חשוב, המשפט נדרש לספק כלים חדשים שיבטיחו את ההגנה על פרטיות המשתמשים. אחת הדוגמאות המרכזיות היא החקיקה האירופית בנושא הגנת הפרטיות – ה-GDPR (General Data Protection Regulation). חקיקה זו מעניקה לאזרחים באיחוד האירופי זכויות נרחבות בנוגע לאופן בו המידע האישי שלהם נאסף ומנוהל, כולל הזכות למחוק מידע (\"הזכות להישכח\") והזכות לקבל מידע על מי משתמש במידע האישי שלהם. חוקים דומים נחקקו במדינות נוספות, אולם במקומות רבים בעולם החקיקה בנושא פרטיות עדיין נמצאת בפיגור לעומת התפתחות הטכנולוגיה. נוסף על כך, קיימת שאלה משפטית מעניינת הנוגעת לאחריותן של החברות שמחזיקות במידע. כאשר מתרחשות פריצות אבטחה או דליפות מידע, נדרשת אחריות משפטית מצד החברות, אולם לעיתים קרובות הפיצוי למשתמשים אינו מספק. חקיקה נוספת נדרשת כדי להבטיח שהזכויות הדיגיטליות של המשתמשים יישמרו.\n",
        "    \"\"\",\n",
        "      \"\"\"\n",
        "    מאמר 4: חוזים חכמים וטכנולוגיית הבלוקצ'יין\n",
        "    טכנולוגיית הבלוקצ'יין יצרה מהפכה בשוק הכלכלי והמשפטי, בעיקר באמצעות החוזים החכמים. בלוקצ'יין הוא פרוטוקול מבוזר המאפשר רישום ואימות של עסקאות בצורה מאובטחת וללא צורך בגורם מתווך. החוזים החכמים, הפועלים על גבי רשתות הבלוקצ'יין, הם תוכנות המתבצעות באופן אוטומטי כאשר מתקיימים תנאים מוסכמים בין הצדדים. כלומר, מדובר בחוזים דיגיטליים שיכולים לאכוף את עצמם. לדוגמה, עסקה בין שני צדדים תתבצע אוטומטית כאשר יתקיימו כל התנאים שנקבעו בחוזה החכם, ללא צורך בהתערבות מצד שלישי כמו עורך דין או נוטריון. החוזים החכמים מהווים שינוי תפיסתי עמוק בעולם המשפט, שכן הם מייתרים את הצורך בתהליכי אכיפה מסורתיים. עם זאת, השימוש בחוזים חכמים מעלה שאלות משפטיות רבות. אחת מהשאלות המרכזיות היא מה קורה במצב של טעות או הונאה בחוזה חכם. מאחר והחוזה מתבצע באופן אוטומטי, לא תמיד יש אפשרות לבטל או לשנות את התנאים. כמו כן, קיימת השאלה האם חוזים חכמים עומדים בדרישות החוק הקיימות בכל הנוגע לכשירות חוזית. תחום נוסף הדורש בחינה משפטית הוא האבטחה של רשתות הבלוקצ'יין עצמן. למרות שהבלוקצ'יין נחשב לטכנולוגיה מאובטחת, היו מקרים בהם התרחשה פריצה לרשתות אלו, מה שהוביל לאובדן כספים ונתונים. נושא האבטחה ימשיך להיות נושא מרכזי בעידן הבלוקצ'יין, וככל שיותר גופים מאמצים טכנולוגיה זו, יידרש פיקוח רגולטורי מתאים. בנוסף, יש לבחון את סוגיות המסים הנוגעות לעסקאות המתבצעות באמצעות בלוקצ'יין, שכן חוזים חכמים מציבים אתגרים חדשים במיסוי ובהגדרה המשפטית של הרווחים הנובעים מהם.\n",
        "    \"\"\"\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdCEv6asASJI"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('database.json', 'r', encoding='utf-8') as file:\n",
        "    json_data = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5yLs63T_tWe"
      },
      "outputs": [],
      "source": [
        "\n",
        "data = [item['data'] for item in json_data]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpB0g4IqD43F"
      },
      "source": [
        "### **Step 2: Vector Database**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIdPItlXERXe"
      },
      "source": [
        "1. Generating Embeddings for All Texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mW7tZpSQEVKw",
        "outputId": "8e57b603-3cab-487c-d9fd-23896093fc7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.00826032  0.08355446  0.01919199 ...  0.01981263 -0.01772506\n",
            "  -0.01057865]\n",
            " [ 0.00197358  0.14719911  0.0248766  ...  0.03754839  0.0019132\n",
            "  -0.05493992]\n",
            " [ 0.01169664  0.05381443 -0.00044901 ...  0.00543916 -0.07542367\n",
            "  -0.04113217]]\n"
          ]
        }
      ],
      "source": [
        "sbert_embeddings = sbert_model.encode(data, convert_to_numpy=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpcCahfEb39"
      },
      "source": [
        "2. Defining the Similarity Matching Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bYQh7jzEeQY"
      },
      "outputs": [],
      "source": [
        "def find_best_match(query, texts, sbert_model, sbert_embeddings, top_k=2):\n",
        "    \"\"\"\n",
        "    Finds the best matching text(s) for the given query based on cosine similarity.\n",
        "\n",
        "    Parameters:\n",
        "    - query (str): The user query string.\n",
        "    - texts (List[str]): List of texts to compare against.\n",
        "    - sbert_model (SentenceTransformer): The Sentence Transformers model.\n",
        "    - sbert_embeddings (np.ndarray): Precomputed embeddings of the texts.\n",
        "    - top_k (int): Number of top matches to return.\n",
        "\n",
        "    Returns:\n",
        "    - List of tuples containing (text, similarity_score)\n",
        "    \"\"\"\n",
        "    query_embedding = sbert_model.encode([query], convert_to_numpy=True)\n",
        "\n",
        "    similarities = cosine_similarity(query_embedding, sbert_embeddings)[0]\n",
        "\n",
        "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "\n",
        "    best_matches = [(texts[idx], similarities[idx]) for idx in top_indices]\n",
        "\n",
        "    return best_matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAReMfTOEpxT"
      },
      "source": [
        "3. testing the model with query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87hT-jmjE4Oj",
        "outputId": "a94ec262-c120-4a71-cce9-bc3a92c536e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ליאור סילברמן מאד אוהב לאכול שניצלים\n",
            "Best Match:\n",
            "Text: ליאור סילברמן מאד אוהב לאכול שניצלים\n",
            "Similarity Score: 0.7990\n",
            "\n",
            "Text: ליאור סילברמן אוהב מאד פיצה\n",
            "Similarity Score: 0.7983\n",
            "\n",
            "Text:  עבור יוסי האוכל בבית מאד טעים\n",
            "Similarity Score: 0.5549\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = \"מה ליאור סילברמן אוהב לאכול?\"\n",
        "\n",
        "# Find the best match\n",
        "best_match = find_best_match(query, data, sbert_model, sbert_embeddings, top_k=5)\n",
        "best_match_text = best_match[0][0]\n",
        "\n",
        "print(best_match_text)\n",
        "\n",
        "# Display the result\n",
        "print(\"Best Match:\")\n",
        "for text, score in best_match:\n",
        "    print(f\"Text: {text}\\nSimilarity Score: {score:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSeqhPkHiHCA"
      },
      "source": [
        "## Try gpt2 LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rx3dT_SAA9eo",
        "outputId": "02b8d954-f028-446e-cb3a-e86fbad39785"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load GPT-2 model and tokenizer\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# # Function to generate a response\n",
        "# def generate_response(text, gpt2_model, gpt2_tokenizer):\n",
        "#     input_ids = gpt2_tokenizer.encode(text, return_tensors='pt')\n",
        "#     output = gpt2_model.generate(input_ids, max_length=150, num_return_sequences=1)\n",
        "#     generated_text = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "#     return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "L1tV73_QBAJN",
        "outputId": "82504384-7928-4018-d75b-ca3fd1683c5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ������ ��ל ������ ��ל ������ ��ל ������ ��ל ������ ��ל ������ ��ל ������ ��"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-817006e6e4d8>\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mgenerated_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_streaming_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_match_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpt2_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpt2_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nFull Generated Response:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-817006e6e4d8>\u001b[0m in \u001b[0;36mgenerate_streaming_response\u001b[0;34m(text, gpt2_model, gpt2_tokenizer, max_new_tokens)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_new_tokens\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set an arbitrary large limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Generate the next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt2_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mnext_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1316\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1127\u001b[0m                 )\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m   1130\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         attn_outputs = self.attn(\n\u001b[0m\u001b[1;32m    615\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;31m# Final projection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresid_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0msize_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def generate_streaming_response(text, gpt2_model, gpt2_tokenizer, max_new_tokens=None):\n",
        "    # Tokenize the input text with truncation and padding\n",
        "    inputs = gpt2_tokenizer(\n",
        "        text,\n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        max_length=1024,\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    # Initialize the input_ids with the tokenized input\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    # List to store the generated tokens\n",
        "    generated_tokens = []\n",
        "\n",
        "    # Generate tokens in a loop\n",
        "    for _ in range(max_new_tokens or 1024):  # Set an arbitrary large limit\n",
        "        # Generate the next token\n",
        "        outputs = gpt2_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        next_token_logits = outputs.logits[:, -1, :]\n",
        "        next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
        "\n",
        "        # Append the new token to the list of generated tokens\n",
        "        generated_tokens.append(next_token_id.item())\n",
        "\n",
        "        # Break the loop if EOS token is generated\n",
        "        if next_token_id.item() == gpt2_tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "        # Concatenate the new token to the input_ids\n",
        "        input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
        "        attention_mask = torch.cat(\n",
        "            [attention_mask, torch.ones_like(next_token_id)], dim=-1\n",
        "        )\n",
        "\n",
        "        # Decode and print the new token\n",
        "        generated_text = gpt2_tokenizer.decode(next_token_id[0], skip_special_tokens=True)\n",
        "        print(generated_text, end='', flush=True)\n",
        "\n",
        "    print()  # Newline at the end\n",
        "\n",
        "    # Decode the full generated text\n",
        "    full_generated_text = gpt2_tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "    return full_generated_text\n",
        "\n",
        "# Example usage\n",
        "generated_response = generate_streaming_response(best_match_text, gpt2_model, gpt2_tokenizer)\n",
        "print(\"\\nFull Generated Response:\\n\", generated_response)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2unJ7vHXiwMy"
      },
      "source": [
        "## Try GenAi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VebDQfSsP2Bm",
        "outputId": "7b35bcf1-35c5-4688-d52b-3c50bfe5bcd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/165.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m163.8/165.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.0/165.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/725.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m725.4/725.4 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mZPwc4SZZwG"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "jrx34SVDPsht",
        "outputId": "0fe8b878-f103-4cca-c6a6-28966434e572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ליאור סילברמן כנראה מאוד אוהב שניצלים ופיצה. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "genai.configure(api_key=userdata.get('API_KEY'))\n",
        "\n",
        "# Construct the prompt\n",
        "prompt = f\"\"\"\n",
        "Generate a comprehensive response based on the following:\n",
        "\n",
        "**Query:** {query}\n",
        "\n",
        "**Most Similar SBERT Answer Embedding:** {best_match}\n",
        "\n",
        "**Response Format:** Informative and concise.\n",
        "\"\"\"\n",
        "\n",
        "# Generate the response\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "# response = genai.GenerativeModel(\"gemini-1.5-pro-001\").generateContent(\n",
        "#     prompt=prompt\n",
        "# )\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm-t7jmnEdpE"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7bRwy4Q-0RN"
      },
      "source": [
        "### load BERT and Tokenizer - option 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "27F3D5Ix-0Ra",
        "outputId": "11c517c7-ad9b-4308-9138-e554a79d4742"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ehsanganim/Desktop/python/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2SvqTIH-0Rc"
      },
      "source": [
        "extract embeddings from BERT using the [CLS] token, which will serve as the sentence embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6oWQ8Ci-0Rc"
      },
      "outputs": [],
      "source": [
        "# Function to get the [CLS] embedding for a given sentence\n",
        "def get_cls_embedding(sentence):\n",
        "    # Tokenize the sentence\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "\n",
        "    # Get the embeddings from the BERT model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Extract the [CLS] token embedding (first token)\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
        "    return cls_embedding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPVjt70W-0Rd"
      },
      "source": [
        "compute the similarity between different queries using cosine similarity to show how BERT handles semantic similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwuE6ZCN-0Rd",
        "outputId": "9892f609-bc7d-42b9-e0a7-fbfd685c2de9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity between query 1 and query 2: 0.8579674363136292\n",
            "Similarity between query 1 and query 3: 0.8106188774108887\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Get embeddings for similar and different queries\n",
        "query1 = \"Explain the role of backpropagation in deep learning.\"\n",
        "query2 = \"How do convolutional neural networks work?\"\n",
        "query3 = \"What is the process to register for courses at the university?\"\n",
        "\n",
        "embedding1 = get_cls_embedding(query1)\n",
        "embedding2 = get_cls_embedding(query2)\n",
        "embedding3 = get_cls_embedding(query3)\n",
        "\n",
        "# Compute cosine similarities\n",
        "similarity_1_2 = cosine_similarity(embedding1, embedding2)\n",
        "similarity_1_3 = cosine_similarity(embedding1, embedding3)\n",
        "\n",
        "print(f\"Similarity between query 1 and query 2: {similarity_1_2[0][0]}\")\n",
        "print(f\"Similarity between query 1 and query 3: {similarity_1_3[0][0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AGcpiiC-0Rd"
      },
      "source": [
        "text retrevial using BERT we embedd the documents with a cls token which respresents the sentence and we then do a similarity check between the query .\n",
        "since the query is about backpropagtion document 1 and 3 should have a higher similarity than doc 2 which is about vpb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbS61ReR-0Rd",
        "outputId": "a7011025-814e-46b8-f4d6-4474996e0e6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity with Document 1: 0.7179515361785889\n",
            "Similarity with Document 2: 0.5887855887413025\n",
            "Similarity with Document 3: 0.6379861831665039\n"
          ]
        }
      ],
      "source": [
        "# Example documents\n",
        "doc1 = \"Neural networks are computing systems inspired by the biological neural networks.\"\n",
        "doc2 = \"To connect to the university VPN, you need to configure your VPN client.\"\n",
        "doc3 = \"Backpropagation is a fundamental algorithm in training deep learning models.\"\n",
        "\n",
        "# Get embeddings for documents\n",
        "doc_embedding1 = get_cls_embedding(doc1)\n",
        "doc_embedding2 = get_cls_embedding(doc2)\n",
        "doc_embedding3 = get_cls_embedding(doc3)\n",
        "\n",
        "# Compare query to documents\n",
        "query = \"How does backpropagation work in neural networks?\"\n",
        "\n",
        "query_embedding = get_cls_embedding(query)\n",
        "\n",
        "# Compute cosine similarities between query and documents\n",
        "similarity_doc1 = cosine_similarity(query_embedding, doc_embedding1)\n",
        "similarity_doc2 = cosine_similarity(query_embedding, doc_embedding2)\n",
        "similarity_doc3 = cosine_similarity(query_embedding, doc_embedding3)\n",
        "\n",
        "# Show results\n",
        "print(f\"Similarity with Document 1: {similarity_doc1[0][0]}\")\n",
        "print(f\"Similarity with Document 2: {similarity_doc2[0][0]}\")\n",
        "print(f\"Similarity with Document 3: {similarity_doc3[0][0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDFCWH6o-0Re"
      },
      "outputs": [],
      "source": [
        "# Example documents\n",
        "doc1 = \"Artificial Intelligence (AI) has transformed the healthcare industry by offering new ways to diagnose, treat, and manage diseases. AI algorithms, particularly deep learning, are being used to analyze medical images, predict disease outbreaks, and personalize treatment plans. The integration of AI in healthcare has reduced human error, improved accuracy, and increased the efficiency of medical professionals. AI-powered robots are assisting surgeons in complex procedures, while predictive analytics is helping doctors make more informed decisions. Despite these advances, there are challenges such as data privacy and the need for comprehensive validation of AI models before widespread adoption.\"\n",
        "doc2 = \"Cloud computing has revolutionized the way businesses operate, offering flexible and scalable infrastructure that can adjust to their needs. By moving to the cloud, companies no longer need to invest heavily in on-premise hardware. Instead, they can access powerful computing resources over the internet, enabling them to focus on innovation and growth. Businesses use cloud services for data storage, application hosting, and collaboration, benefiting from reduced costs, enhanced security, and improved accessibility. However, concerns about data breaches and vendor lock-in persist, as companies need to carefully select cloud providers to ensure long-term sustainability.\"\n",
        "doc3 = \"Neural networks, a fundamental building block of artificial intelligence, have evolved significantly since their inception. Initially inspired by the human brain, neural networks are designed to mimic the way neurons in the brain process information. Over the years, advances in deep learning, a subset of neural networks, have made it possible for AI systems to achieve unprecedented levels of accuracy in tasks like image recognition, natural language processing, and autonomous driving. Neural networks are composed of layers of interconnected nodes, where each node represents a neuron. The training of neural networks involves adjusting weights based on input data, allowing the model to learn patterns and make predictions. Despite their success, training large neural networks requires significant computational power and data.\"\n",
        "\n",
        "# Get embeddings for documents\n",
        "doc_embedding1 = get_cls_embedding(doc1)\n",
        "doc_embedding2 = get_cls_embedding(doc2)\n",
        "doc_embedding3 = get_cls_embedding(doc3)\n",
        "\n",
        "# Compare query to documents\n",
        "query = \"How do neural networks function in artificial intelligence, and what are the challenges of training them?\"\n",
        "\n",
        "query_embedding = get_cls_embedding(query)\n",
        "\n",
        "# Compute cosine similarities between query and documents\n",
        "similarity_doc1 = cosine_similarity(query_embedding, doc_embedding1)\n",
        "similarity_doc2 = cosine_similarity(query_embedding, doc_embedding2)\n",
        "similarity_doc3 = cosine_similarity(query_embedding, doc_embedding3)\n",
        "\n",
        "# Show results\n",
        "print(f\"Similarity with Document 1: {similarity_doc1[0][0]}\")\n",
        "print(f\"Similarity with Document 2: {similarity_doc2[0][0]}\")\n",
        "print(f\"Similarity with Document 3: {similarity_doc3[0][0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbrb_3mu-0Re"
      },
      "source": [
        "as you can see the results are as expected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7beQLll-0Re",
        "outputId": "cf57cf33-5e6f-4882-c4bc-b425a2c96a7c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ehsanganim/Desktop/python/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity with Document 1 (AI in Healthcare): 0.7179515361785889\n",
            "Similarity with Document 2 (Cloud Computing): 0.5887855887413025\n",
            "Similarity with Document 3 (Neural Networks in AI): 0.6379861831665039\n",
            "Similarity with Document 4 (Sustainable Energy): 0.6569727659225464\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Function to get the [CLS] embedding for a given sentence\n",
        "def get_cls_embedding(sentence):\n",
        "    # Tokenize the sentence\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "\n",
        "    # Get the embeddings from the BERT model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Extract the [CLS] token embedding (first token)\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
        "    return cls_embedding\n",
        "\n",
        "# Example documents (longer text)\n",
        "doc1 = \"\"\"\n",
        "Artificial Intelligence (AI) has transformed the healthcare industry by offering new ways to diagnose,\n",
        "treat, and manage diseases. AI algorithms, particularly deep learning, are being used to analyze medical\n",
        "images, predict disease outbreaks, and personalize treatment plans. The integration of AI in healthcare has\n",
        "reduced human error, improved accuracy, and increased the efficiency of medical professionals. AI-powered\n",
        "robots are assisting surgeons in complex procedures, while predictive analytics is helping doctors make\n",
        "more informed decisions. Despite these advances, there are challenges such as data privacy and the need\n",
        "for comprehensive validation of AI models before widespread adoption.\n",
        "\"\"\"\n",
        "\n",
        "doc2 = \"\"\"\n",
        "Cloud computing has revolutionized the way businesses operate, offering flexible and scalable infrastructure\n",
        "that can adjust to their needs. By moving to the cloud, companies no longer need to invest heavily in\n",
        "on-premise hardware. Instead, they can access powerful computing resources over the internet, enabling them\n",
        "to focus on innovation and growth. Businesses use cloud services for data storage, application hosting, and\n",
        "collaboration, benefiting from reduced costs, enhanced security, and improved accessibility. However, concerns\n",
        "about data breaches and vendor lock-in persist, as companies need to carefully select cloud providers to\n",
        "ensure long-term sustainability.\n",
        "\"\"\"\n",
        "\n",
        "doc3 = \"\"\"\n",
        "Neural networks, a fundamental building block of artificial intelligence, have evolved significantly since their\n",
        "inception. Initially inspired by the human brain, neural networks are designed to mimic the way neurons in\n",
        "the brain process information. Over the years, advances in deep learning, a subset of neural networks, have made\n",
        "it possible for AI systems to achieve unprecedented levels of accuracy in tasks like image recognition, natural\n",
        "language processing, and autonomous driving. Neural networks are composed of layers of interconnected nodes,\n",
        "where each node represents a neuron. The training of neural networks involves adjusting weights based on input\n",
        "data, allowing the model to learn patterns and make predictions. Despite their success, training large neural\n",
        "networks requires significant computational power and data.\n",
        "\"\"\"\n",
        "\n",
        "doc4 = \"\"\"\n",
        "As the world faces the growing threat of climate change, sustainable energy has become a major focus of global\n",
        "efforts. Renewable energy sources such as solar, wind, and hydropower are being developed to reduce dependence\n",
        "on fossil fuels. Clean technologies are playing a critical role in achieving sustainability goals, with innovations\n",
        "in energy storage, electric vehicles, and smart grids leading the way. Governments and private companies alike are\n",
        "investing heavily in research and development to create more efficient and cost-effective solutions. While the\n",
        "transition to sustainable energy presents challenges, including the initial cost of infrastructure and the need for\n",
        "reliable energy storage, it also offers immense benefits in terms of reducing greenhouse gas emissions and creating\n",
        "new economic opportunities.\n",
        "\"\"\"\n",
        "\n",
        "# Query to compare with the documents\n",
        "query = \"How do neural networks function in artificial intelligence, and what are the challenges of training them?\"\n",
        "\n",
        "# Get embeddings for documents and query\n",
        "doc_embedding1 = get_cls_embedding(doc1)\n",
        "doc_embedding2 = get_cls_embedding(doc2)\n",
        "doc_embedding3 = get_cls_embedding(doc3)\n",
        "doc_embedding4 = get_cls_embedding(doc4)\n",
        "query_embedding = get_cls_embedding(query)\n",
        "\n",
        "# Compute cosine similarities between the query and each document\n",
        "similarity_doc1 = cosine_similarity(query_embedding, doc_embedding1)\n",
        "similarity_doc2 = cosine_similarity(query_embedding, doc_embedding2)\n",
        "similarity_doc3 = cosine_similarity(query_embedding, doc_embedding3)\n",
        "similarity_doc4 = cosine_similarity(query_embedding, doc_embedding4)\n",
        "\n",
        "# Show similarity results\n",
        "print(f\"Similarity with Document 1 (AI in Healthcare): {similarity_doc1[0][0]}\")\n",
        "print(f\"Similarity with Document 2 (Cloud Computing): {similarity_doc2[0][0]}\")\n",
        "print(f\"Similarity with Document 3 (Neural Networks in AI): {similarity_doc3[0][0]}\")\n",
        "print(f\"Similarity with Document 4 (Sustainable Energy): {similarity_doc4[0][0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy4b4hnY-0Rf"
      },
      "source": [
        "the issue with BERT was he computed based on the word and which had the most occurences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 3: Inserting Data\n",
        "\n",
        "In this step, we worked on inserting transcriptions into a **JSON file** called `data.json` with the following format:\n",
        "\n",
        "```json\n",
        "[\n",
        "  {\"data\": \"Transcription 1...\"},\n",
        "  {\"data\": \"Transcription 2...\"},\n",
        "  {\"data\": \"Transcription 3...\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ### Initialize Whisper Model\n",
        "* We used the `faster_whisper` model to transcribe audio files into text. The transcriptions are done on a chosen device (`CPU` or `CUDA`)    depending on availability.\n",
        "\n",
        " **Transcribing Audio**:\n",
        "   We created a function `transcribe_audio()` that takes an audio file, processes it with Whisper, and returns the transcribed text.\n",
        "\n",
        "   \n",
        "**Handling the `data.json` File**:\n",
        "\n",
        "   * We first checked if the `data.json` file exists and is not empty. If the file is empty or does not exist, we initialized it with an empty list (`[]`).\n",
        "\n",
        "   * If the file exists, we loaded its content using `json.load()`.  We ensured that the content is a list; otherwise, we reinitialized it as an empty list.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Transcription added to data.json!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import json\n",
        "from faster_whisper import WhisperModel\n",
        "import os\n",
        "\n",
        "# Initialize the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Model size and compute type based on device\n",
        "model_size = \"small\"\n",
        "compute_type = \"int8\" if device == \"cpu\" else \"float16\"\n",
        "\n",
        "# Load the Whisper model\n",
        "model = WhisperModel(model_size, device=device, compute_type=compute_type)\n",
        "\n",
        "# Transcribe a new audio file\n",
        "def transcribe_audio(audio_file):\n",
        "    segments, info = model.transcribe(audio_file, beam_size=1)\n",
        "\n",
        "    transcription = \"\"\n",
        "    for segment in segments:\n",
        "        transcription += segment.text\n",
        "\n",
        "    return transcription\n",
        "\n",
        "# Path to the existing JSON file\n",
        "json_file_path = 'data.json'\n",
        "\n",
        "# Check if the JSON file exists and has content\n",
        "if os.path.exists(json_file_path) and os.path.getsize(json_file_path) > 0:\n",
        "    # Load existing data from the JSON file\n",
        "    with open(json_file_path, 'r') as json_file:\n",
        "        try:\n",
        "            json_data = json.load(json_file)\n",
        "            # Ensure json_data is a list, if not initialize it\n",
        "            if not isinstance(json_data, list):\n",
        "                json_data = []\n",
        "        except json.JSONDecodeError:\n",
        "            # If the JSON file is corrupted, initialize it as an empty list\n",
        "            json_data = []\n",
        "else:\n",
        "    # If the file doesn't exist or is empty, create a new list\n",
        "    json_data = []\n",
        "\n",
        "# Transcribe an audio file and append the transcription\n",
        "audio_file = 'test1.mp3'\n",
        "transcription = transcribe_audio(audio_file)\n",
        "\n",
        "# Append the new transcription as a dictionary with the key \"data\"\n",
        "json_data.append({\"data\": transcription})\n",
        "\n",
        "# Write the updated data back to the JSON file\n",
        "with open(json_file_path, 'w') as json_file:\n",
        "    json.dump(json_data, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "# Print success message\n",
        "print(f\"Transcription added to {json_file_path}!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
