{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load BERT and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Video Processing: Object Detection and Audio Transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this notebook, we will process several video clips to:\n",
    "\n",
    "- Perform object detection using different models.\n",
    "- Transcribe audio using different transcription tools.\n",
    "- Measure performance metrics such as processing speed, detection accuracy, and transcription quality.\n",
    "- Compare the performance of different tools.\n",
    "\n",
    "**Tools Used:**\n",
    "\n",
    "- **Object Detection Models**:\n",
    "  - YOLOv8 (Ultralytics)\n",
    "  - YOLOv5 (Ultralytics) \n",
    "  - BLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Install necessary packages for document parsing, YOLO object detection, BLIP and data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T12:07:25.928388Z",
     "start_time": "2024-09-16T12:07:02.762150Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (8.2.93)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from ultralytics) (3.9.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from ultralytics) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from ultralytics) (10.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from ultralytics) (1.14.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from ultralytics) (2.4.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from ultralytics) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from ultralytics) (4.66.5)\n",
      "Requirement already satisfied: psutil in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from ultralytics) (6.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from ultralytics) (2.2.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from ultralytics) (2.0.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.9)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (74.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "^C\n",
      "Requirement already satisfied: opencv-python in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "^C\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: torch in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (0.19.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from torch) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from torch) (74.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from transformers) (3.16.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from requests->transformers) (3.9)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: datasets in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from datasets) (3.16.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from datasets) (0.24.7)\n",
      "Requirement already satisfied: packaging in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.9)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\pycharmprojects\\pythonexcellent\\kaleidooproject\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Install Ultralytics for YOLOv8 and YOLOv5\n",
    "!pip install ultralytics\n",
    "\n",
    "# Install OpenCV for video processing\n",
    "!pip install opencv-python\n",
    "\n",
    "# Install pandas for data manipulation\n",
    "!pip install pandas\n",
    "\n",
    "# Install matplotlib and seaborn for visualization\n",
    "!pip install matplotlib seaborn\n",
    "\n",
    "# Install PyTorch and related libraries for deep learning\n",
    "!pip install torch torchvision torchaudio\n",
    "\n",
    "# Install Hugging Face Transformers for NLP and computer vision models\n",
    "!pip install transformers\n",
    "\n",
    "# Install Hugging Face Datasets for data handling\n",
    "!pip install datasets\n",
    "\n",
    "# Install tqdm for progress bars\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Import necessary libraries and AI models for , object detection, video processing, and data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T12:07:34.763799Z",
     "start_time": "2024-09-16T12:07:25.930378Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Data for analysing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T12:07:34.769494Z",
     "start_time": "2024-09-16T12:07:34.765740Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List of video files\n",
    "video_files = [\n",
    "    'SourcesTests/fruit-and-vegetable-detection.mp4',  # High resolution, good lighting\n",
    "    'SourcesTests/traffic-mini.mp4',  # Medium resolution, low lighting\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Function to extract frames from the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T12:07:42.221079Z",
     "start_time": "2024-09-16T12:07:34.771489Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_frames(video_path, frame_interval=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    success, frame = cap.read()\n",
    "    count = 0\n",
    "\n",
    "    while success:\n",
    "        if count % frame_interval == 0:\n",
    "            frames.append(frame)\n",
    "        success, frame = cap.read()\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "# Example usage\n",
    "video_path = 'SourcesTests/fruit-and-vegetable-detection.mp4'  # Update with your video path\n",
    "frames = extract_frames(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load the BLIP processor and model for Video Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T12:07:51.813415Z",
     "start_time": "2024-09-16T12:07:42.222082Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen in the video does it show the apple for the first time?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Perform Video QA using BLIP\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m blip_answers \u001b[38;5;241m=\u001b[39m \u001b[43mblip_video_qa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m, in \u001b[0;36mblip_video_qa\u001b[1;34m(frames, question)\u001b[0m\n\u001b[0;32m     15\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(image, question, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Generate the answer\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m answer \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m answers\u001b[38;5;241m.\u001b[39mappend((i, answer))  \u001b[38;5;66;03m# Store frame index and answer\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\transformers\\models\\blip\\modeling_blip.py:1187\u001b[0m, in \u001b[0;36mBlipForConditionalGeneration.generate\u001b[1;34m(self, pixel_values, input_ids, attention_mask, interpolate_pos_encoding, **generate_kwargs)\u001b[0m\n\u001b[0;32m   1184\u001b[0m input_ids[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtext_config\u001b[38;5;241m.\u001b[39mbos_token_id\n\u001b[0;32m   1185\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m attention_mask[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1187\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msep_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2021\u001b[0m     )\n\u001b[0;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[0;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2041\u001b[0m     )\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2982\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[0;32m   2979\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m   2981\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2982\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   2984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2985\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\transformers\\models\\blip\\modeling_blip_text.py:871\u001b[0m, in \u001b[0;36mBlipTextLMHeadModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, return_logits, is_decoder, reduction)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 871\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_decoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    887\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    888\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\transformers\\models\\blip\\modeling_blip_text.py:782\u001b[0m, in \u001b[0;36mBlipTextModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, is_decoder)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    780\u001b[0m     embedding_output \u001b[38;5;241m=\u001b[39m encoder_embeds\n\u001b[1;32m--> 782\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    794\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    795\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\transformers\\models\\blip\\modeling_blip_text.py:436\u001b[0m, in \u001b[0;36mBlipTextEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    425\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    426\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    427\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    433\u001b[0m         output_attentions,\n\u001b[0;32m    434\u001b[0m     )\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 436\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\transformers\\models\\blip\\modeling_blip_text.py:368\u001b[0m, in \u001b[0;36mBlipTextLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    366\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    367\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# add cross attentions if we output attention weights\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    373\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs \u001b[38;5;241m+\u001b[39m (present_key_value,)\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\transformers\\pytorch_utils.py:239\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\transformers\\models\\blip\\modeling_blip_text.py:379\u001b[0m, in \u001b[0;36mBlipTextLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    378\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 379\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\transformers\\models\\blip\\modeling_blip_text.py:316\u001b[0m, in \u001b[0;36mBlipTextOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    314\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m    315\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m--> 316\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:202\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\PythonExcellent\\KaleidooProject\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:2576\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2574\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[0;32m   2575\u001b[0m     )\n\u001b[1;32m-> 2576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to perform Video Question Answering using BLIP\n",
    "def blip_video_qa(frames, question):\n",
    "    answers = []\n",
    "\n",
    "    for i, frame in enumerate(frames):\n",
    "        # Convert frame to PIL Image\n",
    "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        # Prepare inputs for the model\n",
    "        inputs = processor(image, question, return_tensors=\"pt\")\n",
    "\n",
    "        # Generate the answer\n",
    "        output = model.generate(**inputs)\n",
    "        answer = processor.decode(output[0], skip_special_tokens=True)\n",
    "        answers.append((i, answer))  # Store frame index and answer\n",
    "\n",
    "    return answers\n",
    "\n",
    "# Define your question\n",
    "question = \"When in the video does it show the apple for the first time?\"\n",
    "\n",
    "# Perform Video QA using BLIP\n",
    "blip_answers = blip_video_qa(frames, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " Measure processing time for BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "blip_answers = blip_video_qa(frames, question)\n",
    "blip_processing_time = time.time() - start_time\n",
    "\n",
    "print(f\"BLIP Processing Time: {blip_processing_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Function for Object Detection in Videos Using YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import io\n",
    "def yolo_v8_detection(video_path, user_object, frame_interval=5):\n",
    "    model = YOLO('yolov8s.pt')  # Load the YOLOv8 model\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = 0\n",
    "    timestamps = []\n",
    "    processing_times = []\n",
    "    class_names = model.names\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count += 1\n",
    "    \n",
    "            if frame_count % frame_interval != 0:\n",
    "                continue\n",
    "    \n",
    "            start_time = time.time()\n",
    "            results = model(frame)\n",
    "            end_time = time.time()\n",
    "            processing_times.append(end_time - start_time)\n",
    "    \n",
    "            for result in results:\n",
    "                boxes = result.boxes\n",
    "                for box in boxes:\n",
    "                    class_id = int(box.cls[0])\n",
    "                    class_name = class_names[class_id]\n",
    "                    if class_name == user_object:\n",
    "                        timestamp = frame_count / fps\n",
    "                        timestamps.append(timestamp)\n",
    "                        break\n",
    "\n",
    "    cap.release()\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "\n",
    "    return {\n",
    "        'timestamps': sorted(set(timestamps)),\n",
    "        'processing_time': avg_processing_time,\n",
    "        'frames_processed': frame_count // frame_interval\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Function for Object Detection in Videos Using YOLOv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-16T12:07:51.822402Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import io\n",
    "def yolo_v5_detection(video_path, user_object, frame_interval=5):\n",
    "    model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = 0\n",
    "    timestamps = []\n",
    "    processing_times = []\n",
    "    class_names = model.names\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count += 1\n",
    "    \n",
    "            if frame_count % frame_interval != 0:\n",
    "                continue\n",
    "    \n",
    "            start_time = time.time()\n",
    "            results = model(frame)\n",
    "            end_time = time.time()\n",
    "            processing_times.append(end_time - start_time)\n",
    "    \n",
    "            # Parse detection results\n",
    "            labels = results.xyxy[0][:, -1].cpu().numpy()\n",
    "            for label in labels:\n",
    "                class_name = class_names[int(label)]\n",
    "                if class_name == user_object:\n",
    "                    timestamp = frame_count / fps\n",
    "                    timestamps.append(timestamp)\n",
    "                    break\n",
    "\n",
    "    cap.release()\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "\n",
    "    return {\n",
    "        'timestamps': sorted(set(timestamps)),\n",
    "        'processing_time': avg_processing_time,\n",
    "        'frames_processed': frame_count // frame_interval\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Script for Running Object Detection and Video Question Answering with YOLOv8, YOLOv5, and BLIP\n",
    "Table of Data analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-16T12:07:51.824402Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_user_objects = {\n",
    "    'SourcesTests/fruit-and-vegetable-detection.mp4': 'apple',\n",
    "    'SourcesTests/traffic-mini.mp4': 'truck',\n",
    "    # Add more mappings if needed\n",
    "}\n",
    "# List to store results\n",
    "results_list = []\n",
    "\n",
    "for video in video_files:\n",
    "    print(f\"Processing {video}...\")\n",
    "    \n",
    "    user_object = video_user_objects.get(video, None)\n",
    "    if user_object is None:\n",
    "        print(f\"No user_object specified for {video}. Skipping this video.\")\n",
    "        continue\n",
    "    \n",
    "    # YOLOv8 Detection\n",
    "    yolo_v8_results = yolo_v8_detection(video, user_object)\n",
    "    \n",
    "    \n",
    "    results_list.append({\n",
    "        'Video': video,\n",
    "        'Tool': 'YOLOv8',\n",
    "        'Processing Time (s)': yolo_v8_results['processing_time'],\n",
    "        'Frames Processed': yolo_v8_results['frames_processed']\n",
    "    })\n",
    "    \n",
    "    # YOLOv5 Detection\n",
    "    yolo_v5_results = yolo_v5_detection(video, user_object)\n",
    "  \n",
    "    \n",
    "    results_list.append({\n",
    "        'Video': video,\n",
    "        'Tool': 'YOLOv5',\n",
    "        'Processing Time (s)': yolo_v5_results['processing_time'],\n",
    "        'Frames Processed': yolo_v5_results['frames_processed']\n",
    "    })\n",
    "    # BLIP Detection (using Video QA)\n",
    "    start_time = time.time()\n",
    "    blip_answers = blip_video_qa(extract_frames(video), \"When in the video its show on the first time the apple?\")\n",
    "    blip_processing_time = time.time() - start_time\n",
    "\n",
    "    results_list.append({\n",
    "        'Video': video,\n",
    "        'Tool': 'BLIP',\n",
    "        'Processing Time (s)': blip_processing_time,\n",
    "        'Frames Processed': len(frames)\n",
    "    })\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    pd.set_option('display.max_columns', None)  # Ensure all columns are displayed\n",
    "    pd.set_option('display.expand_frame_repr', False)  # Avoid line breaks in the table display\n",
    "    pd.set_option('display.colheader_justify', 'center')  # Center-align column headers\n",
    "    \n",
    "    # Print the results DataFrame\n",
    "    print(\"\\nSummary of Processing Results:\")\n",
    "    print(results_df.to_string(index=False))  # Display the DataFrame in a readable format\n",
    "    \n",
    "    # Plot Processing Time Comparison for YOLOv8, YOLOv5, and BLIP\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=results_df, x='Tool', y='Processing Time (s)', hue='Video')\n",
    "    plt.title('Processing Time Comparison for YOLOv8, YOLOv5, and BLIP')\n",
    "    plt.xlabel('Detection Tool')\n",
    "    plt.ylabel('Processing Time (seconds)')\n",
    "    plt.legend(title='Video File', loc='upper right')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "    plt.tight_layout()  # Adjust layout for better fit\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "sBert vs. Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-16T12:07:51.826398Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract embeddings from BERT using the [CLS] token, which will serve as the sentence embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-16T12:07:51.828399Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to get the [CLS] embedding for a given sentence\n",
    "def get_cls_embedding(sentence):\n",
    "    # Tokenize the sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "    \n",
    "    # Get the embeddings from the BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the [CLS] token embedding (first token)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "    return cls_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute the similarity between different queries using cosine similarity to show how BERT handles semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-16T12:07:51.829398Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Get embeddings for similar and different queries\n",
    "query1 = \"Explain the role of backpropagation in deep learning.\"\n",
    "query2 = \"How do convolutional neural networks work?\"\n",
    "query3 = \"What is the process to register for courses at the university?\"\n",
    "\n",
    "embedding1 = get_cls_embedding(query1)\n",
    "embedding2 = get_cls_embedding(query2)\n",
    "embedding3 = get_cls_embedding(query3)\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarity_1_2 = cosine_similarity(embedding1, embedding2)\n",
    "similarity_1_3 = cosine_similarity(embedding1, embedding3)\n",
    "\n",
    "print(f\"Similarity between query 1 and query 2: {similarity_1_2[0][0]}\")\n",
    "print(f\"Similarity between query 1 and query 3: {similarity_1_3[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text retrevial using BERT we embedd the documents with a cls token which respresents the sentence and we then do a similarity check between the query .\n",
    "since the query is about backpropagtion document 1 and 3 should have a higher similarity than doc 2 which is about vpb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example documents\n",
    "doc1 = \"Neural networks are computing systems inspired by the biological neural networks.\"\n",
    "doc2 = \"To connect to the university VPN, you need to configure your VPN client.\"\n",
    "doc3 = \"Backpropagation is a fundamental algorithm in training deep learning models.\"\n",
    "\n",
    "# Get embeddings for documents\n",
    "doc_embedding1 = get_cls_embedding(doc1)\n",
    "doc_embedding2 = get_cls_embedding(doc2)\n",
    "doc_embedding3 = get_cls_embedding(doc3)\n",
    "\n",
    "# Compare query to documents\n",
    "query = \"How does backpropagation work in neural networks?\"\n",
    "\n",
    "query_embedding = get_cls_embedding(query)\n",
    "\n",
    "# Compute cosine similarities between query and documents\n",
    "similarity_doc1 = cosine_similarity(query_embedding, doc_embedding1)\n",
    "similarity_doc2 = cosine_similarity(query_embedding, doc_embedding2)\n",
    "similarity_doc3 = cosine_similarity(query_embedding, doc_embedding3)\n",
    "\n",
    "# Show results\n",
    "print(f\"Similarity with Document 1: {similarity_doc1[0][0]}\")\n",
    "print(f\"Similarity with Document 2: {similarity_doc2[0][0]}\")\n",
    "print(f\"Similarity with Document 3: {similarity_doc3[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T12:12:28.849935Z",
     "start_time": "2024-09-16T12:12:28.813650Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_cls_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m doc3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeural networks, a fundamental building block of artificial intelligence, have evolved significantly since their inception. Initially inspired by the human brain, neural networks are designed to mimic the way neurons in the brain process information. Over the years, advances in deep learning, a subset of neural networks, have made it possible for AI systems to achieve unprecedented levels of accuracy in tasks like image recognition, natural language processing, and autonomous driving. Neural networks are composed of layers of interconnected nodes, where each node represents a neuron. The training of neural networks involves adjusting weights based on input data, allowing the model to learn patterns and make predictions. Despite their success, training large neural networks requires significant computational power and data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Get embeddings for documents\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m doc_embedding1 \u001b[38;5;241m=\u001b[39m \u001b[43mget_cls_embedding\u001b[49m(doc1)\n\u001b[0;32m      8\u001b[0m doc_embedding2 \u001b[38;5;241m=\u001b[39m get_cls_embedding(doc2)\n\u001b[0;32m      9\u001b[0m doc_embedding3 \u001b[38;5;241m=\u001b[39m get_cls_embedding(doc3)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_cls_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "# Example documents\n",
    "doc1 = \"Artificial Intelligence (AI) has transformed the healthcare industry by offering new ways to diagnose, treat, and manage diseases. AI algorithms, particularly deep learning, are being used to analyze medical images, predict disease outbreaks, and personalize treatment plans. The integration of AI in healthcare has reduced human error, improved accuracy, and increased the efficiency of medical professionals. AI-powered robots are assisting surgeons in complex procedures, while predictive analytics is helping doctors make more informed decisions. Despite these advances, there are challenges such as data privacy and the need for comprehensive validation of AI models before widespread adoption.\"\n",
    "doc2 = \"Cloud computing has revolutionized the way businesses operate, offering flexible and scalable infrastructure that can adjust to their needs. By moving to the cloud, companies no longer need to invest heavily in on-premise hardware. Instead, they can access powerful computing resources over the internet, enabling them to focus on innovation and growth. Businesses use cloud services for data storage, application hosting, and collaboration, benefiting from reduced costs, enhanced security, and improved accessibility. However, concerns about data breaches and vendor lock-in persist, as companies need to carefully select cloud providers to ensure long-term sustainability.\"\n",
    "doc3 = \"Neural networks, a fundamental building block of artificial intelligence, have evolved significantly since their inception. Initially inspired by the human brain, neural networks are designed to mimic the way neurons in the brain process information. Over the years, advances in deep learning, a subset of neural networks, have made it possible for AI systems to achieve unprecedented levels of accuracy in tasks like image recognition, natural language processing, and autonomous driving. Neural networks are composed of layers of interconnected nodes, where each node represents a neuron. The training of neural networks involves adjusting weights based on input data, allowing the model to learn patterns and make predictions. Despite their success, training large neural networks requires significant computational power and data.\"\n",
    "\n",
    "# Get embeddings for documents\n",
    "doc_embedding1 = get_cls_embedding(doc1)\n",
    "doc_embedding2 = get_cls_embedding(doc2)\n",
    "doc_embedding3 = get_cls_embedding(doc3)\n",
    "\n",
    "# Compare query to documents\n",
    "query = \"How do neural networks function in artificial intelligence, and what are the challenges of training them?\"\n",
    "\n",
    "query_embedding = get_cls_embedding(query)\n",
    "\n",
    "# Compute cosine similarities between query and documents\n",
    "similarity_doc1 = cosine_similarity(query_embedding, doc_embedding1)\n",
    "similarity_doc2 = cosine_similarity(query_embedding, doc_embedding2)\n",
    "similarity_doc3 = cosine_similarity(query_embedding, doc_embedding3)\n",
    "\n",
    "# Show results\n",
    "print(f\"Similarity with Document 1: {similarity_doc1[0][0]}\")\n",
    "print(f\"Similarity with Document 2: {similarity_doc2[0][0]}\")\n",
    "print(f\"Similarity with Document 3: {similarity_doc3[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see the results are as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to get the [CLS] embedding for a given sentence\n",
    "def get_cls_embedding(sentence):\n",
    "    # Tokenize the sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "    \n",
    "    # Get the embeddings from the BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the [CLS] token embedding (first token)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "    return cls_embedding\n",
    "\n",
    "# Example documents (longer text)\n",
    "doc1 = \"\"\"\n",
    "Artificial Intelligence (AI) has transformed the healthcare industry by offering new ways to diagnose, \n",
    "treat, and manage diseases. AI algorithms, particularly deep learning, are being used to analyze medical \n",
    "images, predict disease outbreaks, and personalize treatment plans. The integration of AI in healthcare has \n",
    "reduced human error, improved accuracy, and increased the efficiency of medical professionals. AI-powered \n",
    "robots are assisting surgeons in complex procedures, while predictive analytics is helping doctors make \n",
    "more informed decisions. Despite these advances, there are challenges such as data privacy and the need \n",
    "for comprehensive validation of AI models before widespread adoption.\n",
    "\"\"\"\n",
    "\n",
    "doc2 = \"\"\"\n",
    "Cloud computing has revolutionized the way businesses operate, offering flexible and scalable infrastructure \n",
    "that can adjust to their needs. By moving to the cloud, companies no longer need to invest heavily in \n",
    "on-premise hardware. Instead, they can access powerful computing resources over the internet, enabling them \n",
    "to focus on innovation and growth. Businesses use cloud services for data storage, application hosting, and \n",
    "collaboration, benefiting from reduced costs, enhanced security, and improved accessibility. However, concerns \n",
    "about data breaches and vendor lock-in persist, as companies need to carefully select cloud providers to \n",
    "ensure long-term sustainability.\n",
    "\"\"\"\n",
    "\n",
    "doc3 = \"\"\"\n",
    "Neural networks, a fundamental building block of artificial intelligence, have evolved significantly since their \n",
    "inception. Initially inspired by the human brain, neural networks are designed to mimic the way neurons in \n",
    "the brain process information. Over the years, advances in deep learning, a subset of neural networks, have made \n",
    "it possible for AI systems to achieve unprecedented levels of accuracy in tasks like image recognition, natural \n",
    "language processing, and autonomous driving. Neural networks are composed of layers of interconnected nodes, \n",
    "where each node represents a neuron. The training of neural networks involves adjusting weights based on input \n",
    "data, allowing the model to learn patterns and make predictions. Despite their success, training large neural \n",
    "networks requires significant computational power and data.\n",
    "\"\"\"\n",
    "\n",
    "doc4 = \"\"\"\n",
    "As the world faces the growing threat of climate change, sustainable energy has become a major focus of global \n",
    "efforts. Renewable energy sources such as solar, wind, and hydropower are being developed to reduce dependence \n",
    "on fossil fuels. Clean technologies are playing a critical role in achieving sustainability goals, with innovations \n",
    "in energy storage, electric vehicles, and smart grids leading the way. Governments and private companies alike are \n",
    "investing heavily in research and development to create more efficient and cost-effective solutions. While the \n",
    "transition to sustainable energy presents challenges, including the initial cost of infrastructure and the need for \n",
    "reliable energy storage, it also offers immense benefits in terms of reducing greenhouse gas emissions and creating \n",
    "new economic opportunities.\n",
    "\"\"\"\n",
    "\n",
    "# Query to compare with the documents\n",
    "query = \"How do neural networks function in artificial intelligence, and what are the challenges of training them?\"\n",
    "\n",
    "# Get embeddings for documents and query\n",
    "doc_embedding1 = get_cls_embedding(doc1)\n",
    "doc_embedding2 = get_cls_embedding(doc2)\n",
    "doc_embedding3 = get_cls_embedding(doc3)\n",
    "doc_embedding4 = get_cls_embedding(doc4)\n",
    "query_embedding = get_cls_embedding(query)\n",
    "\n",
    "# Compute cosine similarities between the query and each document\n",
    "similarity_doc1 = cosine_similarity(query_embedding, doc_embedding1)\n",
    "similarity_doc2 = cosine_similarity(query_embedding, doc_embedding2)\n",
    "similarity_doc3 = cosine_similarity(query_embedding, doc_embedding3)\n",
    "similarity_doc4 = cosine_similarity(query_embedding, doc_embedding4)\n",
    "\n",
    "# Show similarity results\n",
    "print(f\"Similarity with Document 1 (AI in Healthcare): {similarity_doc1[0][0]}\")\n",
    "print(f\"Similarity with Document 2 (Cloud Computing): {similarity_doc2[0][0]}\")\n",
    "print(f\"Similarity with Document 3 (Neural Networks in AI): {similarity_doc3[0][0]}\")\n",
    "print(f\"Similarity with Document 4 (Sustainable Energy): {similarity_doc4[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the issue with BERT was he computed based on the word and which had the most occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-16T12:07:51.839403Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documents Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install -y tesseract-ocr\n",
    "!sudo apt-get install -y tesseract-ocr-heb\n",
    "# Install Python packages\n",
    "!pip install pytesseract\n",
    "!pip install langdetect\n",
    "!pip install Pillow\n",
    "!pip install pandas\n",
    "!pip install PyPDF2\n",
    "!pip install python-pptx\n",
    "!pip install python-docx\n",
    "!pip install pdfminer.six\n",
    "\n",
    "# Verify installed languages\n",
    "!tesseract --list-langs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from google.colab import drive\n",
    "import re\n",
    "from langdetect import detect_langs, DetectorFactory\n",
    "from pptx import Presentation\n",
    "from docx import Document\n",
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the language that i need to extract from image, from extended lang_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure consistent language detection\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Extended language map (detect to Tesseract)\n",
    "lang_map = {\n",
    "    'en': 'eng',    # English\n",
    "    'es': 'spa',    # Spanish\n",
    "    'fr': 'fra',    # French\n",
    "    'de': 'deu',    # German\n",
    "    'it': 'ita',    # Italian\n",
    "    'pt': 'por',    # Portuguese\n",
    "    'ru': 'rus',    # Russian\n",
    "    'zh-cn': 'chi_sim',  # Simplified Chinese\n",
    "    'zh-tw': 'chi_tra',  # Traditional Chinese\n",
    "    'ja': 'jpn',    # Japanese\n",
    "    'ko': 'kor',    # Korean\n",
    "    'ar': 'ara',    # Arabic\n",
    "    'he': 'heb',    # Hebrew\n",
    "    'fa': 'fas',    # Persian (Farsi)\n",
    "    'hi': 'hin',    # Hindi\n",
    "    'th': 'tha',    # Thai\n",
    "    'vi': 'vie',    # Vietnamese\n",
    "    'nl': 'nld',    # Dutch\n",
    "    'tr': 'tur',    # Turkish\n",
    "    'pl': 'pol',    # Polish\n",
    "    'uk': 'ukr',    # Ukrainian\n",
    "    'ro': 'ron',    # Romanian\n",
    "    'bg': 'bul',    # Bulgarian\n",
    "    'el': 'ell',    # Greek\n",
    "    'ur': 'urd',    # Urdu\n",
    "    # Add more languages as needed\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraxt txt, PDF, docs, pptx files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt_file(file_path, encoding='utf-8'):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding=encoding) as file:\n",
    "            lines = [line.rstrip('\\n') for line in file]\n",
    "        return lines\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "    except IOError as e:\n",
    "        print(f\"An I/O error occurred: {e}\")\n",
    "    return []\n",
    "\n",
    "def read_pdf_with_pdfminer(file_path):\n",
    "    try:\n",
    "        text = extract_text(file_path)\n",
    "        lines = text.splitlines()\n",
    "        return lines\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return []\n",
    "\n",
    "def read_pptx_file(file_path):\n",
    "    try:\n",
    "        prs = Presentation(file_path)\n",
    "        text_runs = []\n",
    "        for slide in prs.slides:\n",
    "            for shape in slide.shapes:\n",
    "                if hasattr(shape, \"text\"):\n",
    "                    text_runs.append(shape.text)\n",
    "        return text_runs\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return []\n",
    "\n",
    "def read_docx_file(file_path):\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "        text = []\n",
    "        for para in doc.paragraphs:\n",
    "            text.append(para.text)\n",
    "        return text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract txt from image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Preprocesses an image to improve OCR accuracy.\n",
    "\n",
    "    Steps:\n",
    "    - Convert to grayscale\n",
    "    - Apply median filter for noise reduction\n",
    "    - Enhance contrast\n",
    "    - Binarize the image using thresholding\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "        image = image.filter(ImageFilter.MedianFilter())  # Reduce noise\n",
    "        enhancer = ImageEnhance.Contrast(image)\n",
    "        image = enhancer.enhance(2)  # Enhance contrast\n",
    "        image = image.point(lambda x: 0 if x < 140 else 255, '1')  # Binarization\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error preprocessing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_text_multiple_languages(image, languages=['eng', 'heb']):\n",
    "    \"\"\"\n",
    "    Extracts text from an image using Tesseract OCR with multiple languages.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image): Preprocessed image.\n",
    "        languages (list): List of language codes to use for OCR.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Join language codes with '+' for Tesseract\n",
    "        lang_param = '+'.join(languages)\n",
    "        text = pytesseract.image_to_string(image, lang=lang_param)\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during OCR with multiple languages: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def further_clean_text(text):\n",
    "    \"\"\"\n",
    "    Further cleans the extracted text by removing unwanted characters.\n",
    "\n",
    "    Args:\n",
    "        text (str): Extracted text.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    # Remove non-printable characters\n",
    "    text = ''.join(filter(lambda x: x.isprintable(), text))\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Supported image formats\n",
    "image_extensions = ['*.png', '*.jpg', '*.jpeg', '*.tiff', '*.bmp']\n",
    "\n",
    "# Gather all image file paths\n",
    "image_paths = ['/content/Screenshot 2024-09-16 at 11.59.44.png']\n",
    "\n",
    "\n",
    "print(f\"Found {len(image_paths)} images.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Func to define the file type to ectract the txt from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Initialize lists to store results\n",
    "extracted_text = []\n",
    "detected_file_types = []\n",
    "file_names = []\n",
    "\n",
    "# Define desired languages for OCR\n",
    "desired_languages = ['eng', 'heb', 'spa']  # Add more as needed\n",
    "\n",
    "def extract_text_from_file(file_path, use_pdfminer=False):\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    file_extension = file_extension.lower()\n",
    "\n",
    "    if file_extension == '.txt':\n",
    "        text = read_txt_file(file_path)\n",
    "        return text, \"txt\"\n",
    "\n",
    "    elif file_extension == '.pdf':\n",
    "        if use_pdfminer:\n",
    "            text = read_pdf_with_pdfminer(file_path)\n",
    "        else:\n",
    "            # If you have another PDF reader, integrate it here\n",
    "            text = read_pdf_with_pdfminer(file_path)\n",
    "        return text, \"pdf\"\n",
    "\n",
    "    elif file_extension == '.pptx':\n",
    "        text = read_pptx_file(file_path)\n",
    "        return text, \"pptx\"\n",
    "\n",
    "    elif file_extension == '.docx':\n",
    "        text = read_docx_file(file_path)\n",
    "        return text, \"docx\"\n",
    "\n",
    "    elif file_extension in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']:\n",
    "        # Process image with OCR\n",
    "        preprocessed_img = preprocess_image(file_path)\n",
    "        if preprocessed_img is None:\n",
    "            return \"\", \"preprocessing_failed\"\n",
    "        text = extract_text_multiple_languages(preprocessed_img, languages=desired_languages)\n",
    "        return text, \"image\"\n",
    "\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {file_extension}\")\n",
    "        return \"\", \"unsupported\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main part we put the file path into (uploaded list) and extract the txt from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your list of files to process\n",
    "uploaded = ['/content/Screenshot 2024-09-16 at 11.59.44.png','/content/.docx','/content/Full Stack Project.pdf']\n",
    "\n",
    "# Process all uploaded files\n",
    "for idx, filename in enumerate(uploaded):\n",
    "    print(f\"Processing File {idx + 1}/{len(uploaded)}: {filename}\")\n",
    "    try:\n",
    "        text, file_type = extract_text_from_file(filename, use_pdfminer=True)\n",
    "        if text:\n",
    "            # For text files, text may be a list of lines; for images, text is a string\n",
    "            if isinstance(text, list):\n",
    "                cleaned_text = \"\\n\".join([further_clean_text(line) for line in text])\n",
    "            else:\n",
    "                cleaned_text = further_clean_text(text)\n",
    "            extracted_text.append(cleaned_text)\n",
    "            detected_file_types.append(file_type)\n",
    "            file_names.append(os.path.basename(filename))\n",
    "\n",
    "            # Print a preview of the extracted text\n",
    "            #preview = cleaned_text[:100].replace('\\n', ' ') + ('...' if len(cleaned_text) > 100 else '')\n",
    "        \n",
    "            #print the extracted txt\n",
    "            print(f\"Extracted Text Preview:\")\n",
    "            print(cleaned_text)\n",
    "        else:\n",
    "            print(\"No text extracted.\")\n",
    "            extracted_text.append(\"\")\n",
    "            detected_file_types.append(\"no_text_extracted\")\n",
    "            file_names.append(os.path.basename(filename))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")\n",
    "        extracted_text.append(\"\")\n",
    "        detected_file_types.append(\"error\")\n",
    "        file_names.append(os.path.basename(filename))\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
