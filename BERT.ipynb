{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load BERT and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ehsanganim/Desktop/python/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract embeddings from BERT using the [CLS] token, which will serve as the sentence embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the [CLS] embedding for a given sentence\n",
    "def get_cls_embedding(sentence):\n",
    "    # Tokenize the sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "    \n",
    "    # Get the embeddings from the BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the [CLS] token embedding (first token)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "    return cls_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute the similarity between different queries using cosine similarity to show how BERT handles semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between query 1 and query 2: 0.8579674363136292\n",
      "Similarity between query 1 and query 3: 0.8106188774108887\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Get embeddings for similar and different queries\n",
    "query1 = \"Explain the role of backpropagation in deep learning.\"\n",
    "query2 = \"How do convolutional neural networks work?\"\n",
    "query3 = \"What is the process to register for courses at the university?\"\n",
    "\n",
    "embedding1 = get_cls_embedding(query1)\n",
    "embedding2 = get_cls_embedding(query2)\n",
    "embedding3 = get_cls_embedding(query3)\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarity_1_2 = cosine_similarity(embedding1, embedding2)\n",
    "similarity_1_3 = cosine_similarity(embedding1, embedding3)\n",
    "\n",
    "print(f\"Similarity between query 1 and query 2: {similarity_1_2[0][0]}\")\n",
    "print(f\"Similarity between query 1 and query 3: {similarity_1_3[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text retrevial using BERT we embedd the documents with a cls token which respresents the sentence and we then do a similarity check between the query .\n",
    "since the query is about backpropagtion document 1 and 3 should have a higher similarity than doc 2 which is about vpb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity with Document 1: 0.7179515361785889\n",
      "Similarity with Document 2: 0.5887855887413025\n",
      "Similarity with Document 3: 0.6379861831665039\n"
     ]
    }
   ],
   "source": [
    "# Example documents\n",
    "doc1 = \"Neural networks are computing systems inspired by the biological neural networks.\"\n",
    "doc2 = \"To connect to the university VPN, you need to configure your VPN client.\"\n",
    "doc3 = \"Backpropagation is a fundamental algorithm in training deep learning models.\"\n",
    "\n",
    "# Get embeddings for documents\n",
    "doc_embedding1 = get_cls_embedding(doc1)\n",
    "doc_embedding2 = get_cls_embedding(doc2)\n",
    "doc_embedding3 = get_cls_embedding(doc3)\n",
    "\n",
    "# Compare query to documents\n",
    "query = \"How does backpropagation work in neural networks?\"\n",
    "\n",
    "query_embedding = get_cls_embedding(query)\n",
    "\n",
    "# Compute cosine similarities between query and documents\n",
    "similarity_doc1 = cosine_similarity(query_embedding, doc_embedding1)\n",
    "similarity_doc2 = cosine_similarity(query_embedding, doc_embedding2)\n",
    "similarity_doc3 = cosine_similarity(query_embedding, doc_embedding3)\n",
    "\n",
    "# Show results\n",
    "print(f\"Similarity with Document 1: {similarity_doc1[0][0]}\")\n",
    "print(f\"Similarity with Document 2: {similarity_doc2[0][0]}\")\n",
    "print(f\"Similarity with Document 3: {similarity_doc3[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example documents\n",
    "doc1 = \"Artificial Intelligence (AI) has transformed the healthcare industry by offering new ways to diagnose, treat, and manage diseases. AI algorithms, particularly deep learning, are being used to analyze medical images, predict disease outbreaks, and personalize treatment plans. The integration of AI in healthcare has reduced human error, improved accuracy, and increased the efficiency of medical professionals. AI-powered robots are assisting surgeons in complex procedures, while predictive analytics is helping doctors make more informed decisions. Despite these advances, there are challenges such as data privacy and the need for comprehensive validation of AI models before widespread adoption.\"\n",
    "doc2 = \"Cloud computing has revolutionized the way businesses operate, offering flexible and scalable infrastructure that can adjust to their needs. By moving to the cloud, companies no longer need to invest heavily in on-premise hardware. Instead, they can access powerful computing resources over the internet, enabling them to focus on innovation and growth. Businesses use cloud services for data storage, application hosting, and collaboration, benefiting from reduced costs, enhanced security, and improved accessibility. However, concerns about data breaches and vendor lock-in persist, as companies need to carefully select cloud providers to ensure long-term sustainability.\"\n",
    "doc3 = \"Neural networks, a fundamental building block of artificial intelligence, have evolved significantly since their inception. Initially inspired by the human brain, neural networks are designed to mimic the way neurons in the brain process information. Over the years, advances in deep learning, a subset of neural networks, have made it possible for AI systems to achieve unprecedented levels of accuracy in tasks like image recognition, natural language processing, and autonomous driving. Neural networks are composed of layers of interconnected nodes, where each node represents a neuron. The training of neural networks involves adjusting weights based on input data, allowing the model to learn patterns and make predictions. Despite their success, training large neural networks requires significant computational power and data.\"\n",
    "\n",
    "# Get embeddings for documents\n",
    "doc_embedding1 = get_cls_embedding(doc1)\n",
    "doc_embedding2 = get_cls_embedding(doc2)\n",
    "doc_embedding3 = get_cls_embedding(doc3)\n",
    "\n",
    "# Compare query to documents\n",
    "query = \"How do neural networks function in artificial intelligence, and what are the challenges of training them?\"\n",
    "\n",
    "query_embedding = get_cls_embedding(query)\n",
    "\n",
    "# Compute cosine similarities between query and documents\n",
    "similarity_doc1 = cosine_similarity(query_embedding, doc_embedding1)\n",
    "similarity_doc2 = cosine_similarity(query_embedding, doc_embedding2)\n",
    "similarity_doc3 = cosine_similarity(query_embedding, doc_embedding3)\n",
    "\n",
    "# Show results\n",
    "print(f\"Similarity with Document 1: {similarity_doc1[0][0]}\")\n",
    "print(f\"Similarity with Document 2: {similarity_doc2[0][0]}\")\n",
    "print(f\"Similarity with Document 3: {similarity_doc3[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see the results are as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ehsanganim/Desktop/python/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity with Document 1 (AI in Healthcare): 0.7179515361785889\n",
      "Similarity with Document 2 (Cloud Computing): 0.5887855887413025\n",
      "Similarity with Document 3 (Neural Networks in AI): 0.6379861831665039\n",
      "Similarity with Document 4 (Sustainable Energy): 0.6569727659225464\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to get the [CLS] embedding for a given sentence\n",
    "def get_cls_embedding(sentence):\n",
    "    # Tokenize the sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "    \n",
    "    # Get the embeddings from the BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the [CLS] token embedding (first token)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "    return cls_embedding\n",
    "\n",
    "# Example documents (longer text)\n",
    "doc1 = \"\"\"\n",
    "Artificial Intelligence (AI) has transformed the healthcare industry by offering new ways to diagnose, \n",
    "treat, and manage diseases. AI algorithms, particularly deep learning, are being used to analyze medical \n",
    "images, predict disease outbreaks, and personalize treatment plans. The integration of AI in healthcare has \n",
    "reduced human error, improved accuracy, and increased the efficiency of medical professionals. AI-powered \n",
    "robots are assisting surgeons in complex procedures, while predictive analytics is helping doctors make \n",
    "more informed decisions. Despite these advances, there are challenges such as data privacy and the need \n",
    "for comprehensive validation of AI models before widespread adoption.\n",
    "\"\"\"\n",
    "\n",
    "doc2 = \"\"\"\n",
    "Cloud computing has revolutionized the way businesses operate, offering flexible and scalable infrastructure \n",
    "that can adjust to their needs. By moving to the cloud, companies no longer need to invest heavily in \n",
    "on-premise hardware. Instead, they can access powerful computing resources over the internet, enabling them \n",
    "to focus on innovation and growth. Businesses use cloud services for data storage, application hosting, and \n",
    "collaboration, benefiting from reduced costs, enhanced security, and improved accessibility. However, concerns \n",
    "about data breaches and vendor lock-in persist, as companies need to carefully select cloud providers to \n",
    "ensure long-term sustainability.\n",
    "\"\"\"\n",
    "\n",
    "doc3 = \"\"\"\n",
    "Neural networks, a fundamental building block of artificial intelligence, have evolved significantly since their \n",
    "inception. Initially inspired by the human brain, neural networks are designed to mimic the way neurons in \n",
    "the brain process information. Over the years, advances in deep learning, a subset of neural networks, have made \n",
    "it possible for AI systems to achieve unprecedented levels of accuracy in tasks like image recognition, natural \n",
    "language processing, and autonomous driving. Neural networks are composed of layers of interconnected nodes, \n",
    "where each node represents a neuron. The training of neural networks involves adjusting weights based on input \n",
    "data, allowing the model to learn patterns and make predictions. Despite their success, training large neural \n",
    "networks requires significant computational power and data.\n",
    "\"\"\"\n",
    "\n",
    "doc4 = \"\"\"\n",
    "As the world faces the growing threat of climate change, sustainable energy has become a major focus of global \n",
    "efforts. Renewable energy sources such as solar, wind, and hydropower are being developed to reduce dependence \n",
    "on fossil fuels. Clean technologies are playing a critical role in achieving sustainability goals, with innovations \n",
    "in energy storage, electric vehicles, and smart grids leading the way. Governments and private companies alike are \n",
    "investing heavily in research and development to create more efficient and cost-effective solutions. While the \n",
    "transition to sustainable energy presents challenges, including the initial cost of infrastructure and the need for \n",
    "reliable energy storage, it also offers immense benefits in terms of reducing greenhouse gas emissions and creating \n",
    "new economic opportunities.\n",
    "\"\"\"\n",
    "\n",
    "# Query to compare with the documents\n",
    "query = \"How do neural networks function in artificial intelligence, and what are the challenges of training them?\"\n",
    "\n",
    "# Get embeddings for documents and query\n",
    "doc_embedding1 = get_cls_embedding(doc1)\n",
    "doc_embedding2 = get_cls_embedding(doc2)\n",
    "doc_embedding3 = get_cls_embedding(doc3)\n",
    "doc_embedding4 = get_cls_embedding(doc4)\n",
    "query_embedding = get_cls_embedding(query)\n",
    "\n",
    "# Compute cosine similarities between the query and each document\n",
    "similarity_doc1 = cosine_similarity(query_embedding, doc_embedding1)\n",
    "similarity_doc2 = cosine_similarity(query_embedding, doc_embedding2)\n",
    "similarity_doc3 = cosine_similarity(query_embedding, doc_embedding3)\n",
    "similarity_doc4 = cosine_similarity(query_embedding, doc_embedding4)\n",
    "\n",
    "# Show similarity results\n",
    "print(f\"Similarity with Document 1 (AI in Healthcare): {similarity_doc1[0][0]}\")\n",
    "print(f\"Similarity with Document 2 (Cloud Computing): {similarity_doc2[0][0]}\")\n",
    "print(f\"Similarity with Document 3 (Neural Networks in AI): {similarity_doc3[0][0]}\")\n",
    "print(f\"Similarity with Document 4 (Sustainable Energy): {similarity_doc4[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the issue with BERT was he computed based on the word and which had the most occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
